[{"authors":["admin"],"categories":null,"content":"Matthew Turk is an assistant professor in the School of Information Sciences and also holds an appointment with the Department of Astronomy in the College of Liberal Arts and Sciences. His research is focused on how individuals interact with data and how that data is processed and understood.\nAt the University of Illinois, he leads the Data Exploration Lab and teaches in Data Visualization, Data Storytelling, and Computational Astrophysics.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1559268143,"objectID":"598b63dd58b43bce02403646f240cd3c","permalink":"https://matthewturk.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"author","summary":"Matthew Turk is an assistant professor in the School of Information Sciences and also holds an appointment with the Department of Astronomy in the College of Liberal Arts and Sciences. His research is focused on how individuals interact with data and how that data is processed and understood.\nAt the University of Illinois, he leads the Data Exploration Lab and teaches in Data Visualization, Data Storytelling, and Computational Astrophysics.","tags":null,"title":"Matthew Turk","type":"author"},{"authors":[],"categories":[],"content":" SIDE NOTE: I intended for this blog post to be a bit shorter than it turned out, and for it to cover some things it \u0026hellip; didn\u0026rsquo;t! So it looks like there\u0026rsquo;ll be a part three in the series.\nOperations on Data Objects In my previous post, I walked through a few aspects of how the chunking system in yt works, mostly focusing on the \u0026quot;io\u0026quot; style of chunking, where the order in which data arrives is not important. This style of chunking lends itself very easily to parallelism, as well as dynamic chunk-sizing; we can see this in how operations such as .max() operate on a data object in yt.\nimport yt ds = yt.load(\u0026quot;data/IsolatedGalaxy/galaxy0030/galaxy0030\u0026quot;) dd = ds.r[:,:,:]  yt : [INFO ] 2019-06-10 12:59:13,433 Parameters: current_time = 0.0060000200028298 yt : [INFO ] 2019-06-10 12:59:13,434 Parameters: domain_dimensions = [32 32 32] yt : [INFO ] 2019-06-10 12:59:13,435 Parameters: domain_left_edge = [0. 0. 0.] yt : [INFO ] 2019-06-10 12:59:13,437 Parameters: domain_right_edge = [1. 1. 1.] yt : [INFO ] 2019-06-10 12:59:13,439 Parameters: cosmological_simulation = 0.0 Parsing Hierarchy : 100%|██████████| 173/173 [00:00\u0026lt;00:00, 5931.42it/s] yt : [INFO ] 2019-06-10 12:59:13,487 Gathering a field list (this may take a moment.)  max_vals = dd.max([\u0026quot;density\u0026quot;, \u0026quot;temperature\u0026quot;, \u0026quot;velocity_magnitude\u0026quot;]) print(max_vals)  (7.73426503924e-24 g/cm**3, 24826104.0 K, 86290042.8768639 cm/s)  I want to highlight something here which sometimes slips by \u0026ndash; if you were to access the array hanging off an object like dd, like this:\ndd[\u0026quot;density\u0026quot;]  YTArray([4.92775113e-31, 4.94005233e-31, 4.93824694e-31, ..., 1.12879234e-25, 1.59561490e-25, 1.09824903e-24]) g/cm**3  The entire array is loaded into memory. This is done through a different chunk style, \u0026quot;all\u0026quot;, which pre-allocates an array and then loads the whole thing into memory in a single go. I should note that there are a few reasons that this needs to always be provided in the same order as the \u0026quot;io\u0026quot; chunking style, which has presented some fun struggles in refactoring that I may mention later on.\nBut above, we aren\u0026rsquo;t accessing dd[\u0026quot;density\u0026quot;].max() and instead are accessing dd.max(\u0026quot;density\u0026quot;) (along with two other fields, temperature and the magnitude of velocity.) This operation, inspired by the numpy / xarray syntax, iterates over chunks and computes the running max.\nThere are a few other fun operations that I mentioned last time like argmax and whatnot, but for now let\u0026rsquo;s just look at max. While the syntactic sugar for calling dd.max() is reasonably recent, the underpinning functionality dates back about a decade and has, from the start, been MPI-parallel. It hasn\u0026rsquo;t always been elegant, but it\u0026rsquo;s been parallel and memory-conservative.\nSo how does .max() (and, its older form, .quantities[\u0026quot;MaximumValue\u0026quot;]()) work? Let\u0026rsquo;s take a look at the source.\ndd.max??  Signature: dd.max(field, axis=None) Source: def max(self, field, axis=None): r\u0026quot;\u0026quot;\u0026quot;Compute the maximum of a field, optionally along an axis. This will, in a parallel-aware fashion, compute the maximum of the given field. Supplying an axis will result in a return value of a YTProjection, with method 'mip' for maximum intensity. If the max has already been requested, it will use the cached extrema value. Parameters ---------- field : string or tuple field name The field to maximize. axis : string, optional If supplied, the axis to project the maximum along. Returns ------- Either a scalar or a YTProjection. Examples -------- \u0026gt;\u0026gt;\u0026gt; max_temp = reg.max(\u0026quot;temperature\u0026quot;) \u0026gt;\u0026gt;\u0026gt; max_temp_proj = reg.max(\u0026quot;temperature\u0026quot;, axis=\u0026quot;x\u0026quot;) \u0026quot;\u0026quot;\u0026quot; if axis is None: rv = () fields = ensure_list(field) for f in fields: rv += (self._compute_extrema(f)[\u001b[0;36m1],) if len(fields) == \u001b[0;36m1: return rv[\u001b[0;36m0] else: return rv elif axis in self.ds.coordinates.axis_name: r = self.ds.proj(field, axis, data_source=self, method=\u0026quot;mip\u0026quot;) return r else: raise NotImplementedError(\u0026quot;Unknown axis %s\u0026quot; % axis) File: ~/yt/yt/yt/data_objects/data_containers.py Type: method  (One fun bit here is that if you supply an axis and it\u0026rsquo;s a spatial axis, this will project along the axis.)\nLooks like it calls ._compute_extrema so let\u0026rsquo;s take a look there:\ndd._compute_extrema??  Signature: dd._compute_extrema(field) Docstring: \u0026lt;no docstring\u0026gt; Source: def _compute_extrema(self, field): if self._extrema_cache is None: self._extrema_cache = {} if field not in self._extrema_cache: # Note we still need to call extrema for each field, as of right # now mi, ma = self.quantities.extrema(field) self._extrema_cache[field] = (mi, ma) return self._extrema_cache[field] File: ~/yt/yt/yt/data_objects/data_containers.py Type: method  (Fun fact: until I saw the source code right now, I was prepared to say that it computed all the extrema in a single go. Glad there\u0026rsquo;s a backspace key. I should probably file an issue.)\nThis calls self.quantities.extrema for each field, since it\u0026rsquo;s nearly just as cheap to do both min and max in a single pass, and sometimes folks\u0026rsquo;ll want both.\nSo we\u0026rsquo;re starting to see the underpinnings here \u0026ndash; .quantities is where lots of the fun things happen. What is it?\ndd.quantities.extrema??  Signature: dd.quantities.extrema(fields, non_zero=False) Type: Extrema String form: \u0026lt;yt.data_objects.derived_quantities.Extrema object at 0x7db454d7a2e8\u0026gt; File: ~/yt/yt/yt/data_objects/derived_quantities.py Source: class Extrema(DerivedQuantity): r\u0026quot;\u0026quot;\u0026quot; Calculates the min and max value of a field or list of fields. Returns a YTArray for each field requested. If one, a single YTArray is returned, if many, a list of YTArrays in order of field list is returned. The first element of each YTArray is the minimum of the field and the second is the maximum of the field. Parameters ---------- fields The field or list of fields over which the extrema are to be calculated. non_zero : bool If True, only positive values are considered in the calculation. Default: False Examples -------- \u0026gt;\u0026gt;\u0026gt; ds = load(\u0026quot;IsolatedGalaxy/galaxy0030/galaxy0030\u0026quot;) \u0026gt;\u0026gt;\u0026gt; ad = ds.all_data() \u0026gt;\u0026gt;\u0026gt; print ad.quantities.extrema([(\u0026quot;gas\u0026quot;, \u0026quot;density\u0026quot;), ... (\u0026quot;gas\u0026quot;, \u0026quot;temperature\u0026quot;)]) \u0026quot;\u0026quot;\u0026quot; def count_values(self, fields, non_zero): self.num_vals = len(fields) * \u001b[0;36m2 def __call__(self, fields, non_zero = False): fields = ensure_list(fields) rv = super(Extrema, self).__call__(fields, non_zero) if len(rv) == \u001b[0;36m1: rv = rv[\u001b[0;36m0] return rv def process_chunk(self, data, fields, non_zero): vals = [] for field in fields: field = data._determine_fields(field)[\u001b[0;36m0] fd = data[field] if non_zero: fd = fd[fd \u0026gt; \u001b[0;36m0.0] if fd.size \u0026gt; \u001b[0;36m0: vals += [fd.min(), fd.max()] else: vals += [array_like_field(data, HUGE, field), array_like_field(data, -HUGE, field)] return vals def reduce_intermediate(self, values): # The values get turned into arrays here. return [self.data_source.ds.arr([mis.min(), mas.max()]) for mis, mas in zip(values[::\u001b[0;36m2], values[\u001b[0;36m1::\u001b[0;36m2])] Call docstring: Calculate results for the derived quantity  Ah, this is starting to make sense!\nAll the DerivedQuantity objects\nWhat all do we have?\ndd.quantities.keys()  dict_keys(['WeightedAverageQuantity', 'TotalQuantity', 'TotalMass', 'CenterOfMass', 'BulkVelocity', 'WeightedVariance', 'AngularMomentumVector', 'Extrema', 'SampleAtMaxFieldValues', 'MaxLocation', 'SampleAtMinFieldValues', 'MinLocation', 'SpinParameter'])  Looking at these, there\u0026rsquo;s likely a common theme that is pretty obvious \u0026ndash; they\u0026rsquo;re all pretty easily parallelizable things! Sure, there might need to be some reductions at the end, but these are all pretty straightforward combinations of fields and parameters.\nThe way the base class works is interesting, and we can use that to break down what is going on here in a way that demonstrates how this relies on chunking:\nyt.data_objects.derived_quantities.DerivedQuantity??  Init signature: yt.data_objects.derived_quantities.DerivedQuantity(data_source) Docstring: \u0026lt;no docstring\u0026gt; Source: class DerivedQuantity(ParallelAnalysisInterface): num_vals = -\u001b[0;36m1 def __init__(self, data_source): self.data_source = data_source def count_values(self, *args, **kwargs): return def __call__(self, *args, **kwargs): \u0026quot;\u0026quot;\u0026quot;Calculate results for the derived quantity\u0026quot;\u0026quot;\u0026quot; # create the index if it doesn't exist yet self.data_source.ds.index self.count_values(*args, **kwargs) chunks = self.data_source.chunks([], chunking_style=\u0026quot;io\u0026quot;) storage = {} for sto, ds in parallel_objects(chunks, -\u001b[0;36m1, storage = storage): sto.result = self.process_chunk(ds, *args, **kwargs) # Now storage will have everything, and will be done via pickling, so # the units will be preserved. (Credit to Nathan for this # idea/implementation.) values = [ [] for i in range(self.num_vals) ] for key in sorted(storage): for i in range(self.num_vals): values[i].append(storage[key][i]) # These will be YTArrays values = [self.data_source.ds.arr(values[i]) for i in range(self.num_vals)] values = self.reduce_intermediate(values) return values def process_chunk(self, data, *args, **kwargs): raise NotImplementedError def reduce_intermediate(self, values): raise NotImplementedError File: ~/yt/yt/yt/data_objects/derived_quantities.py Type: RegisteredDerivedQuantity Subclasses: WeightedAverageQuantity, TotalQuantity, CenterOfMass, BulkVelocity, WeightedVariance, AngularMomentumVector, Extrema, SampleAtMaxFieldValues, SpinParameter  The key thing I want to highlight here is that this is rather simple in concept; the chunks are iterated over in parallel (via the parallel_objects routine, which parcels them out to different processors), processed, and then the reduction happens through reduce_intermediate.\nThere are a few things to note here \u0026ndash; this is actually units-aware, which means that even if you\u0026rsquo;ve got (for some reason) cm for a quantity on one processor and km on another, it will correctly convert them. The other is that the set up is such that only the process_chunk and reduce_intermediate operations need to be implemented, along with setting some properties.\nBut, we\u0026rsquo;re getting a bit far away from the topic at hand, which is why how chunking is set up can cause some issues with exposing data to dask. And so I want to return to the notion of the \u0026quot;io\u0026quot; chunking and how this works for differently indexed datasets.\nFine- and Coarse-grained Indexing What yt does during the selection of data is key to how it thinks about the processings of that data. The way that data can be provided to yt takes several forms:\n Regularly structured grids and grid based data, where there may be overlapping regions (typically with one \u0026ldquo;authoritative source of truth\u0026rdquo; as in adaptive mesh refinement) Irregular grids, where the distance between points may vary along each spatial axis Unstructured mesh, where the data arrives in hexahedra, tetrahedra, etc, and there is typically a well-defined form for evaluating field values internal to each polyhedra Discrete, or particle-based datasets, where each point is sampled at some location that we don\u0026rsquo;t know a priori \u0026ndash; for instance, N-body simulations Octree or block-structured data, which can in some cases be thought of as a special-case of grid based data but that follows a more regular form  Several of these have a common trait that comes in quite handy for yt \u0026ndash; namely, that the index of the data occupies considerably less memory than the data itself.\nGrid Indexing For instance, when dealing with a grid of data, typically that grid can be defined by a set of properties such as:\n \u0026ldquo;origin\u0026rdquo; corner of the grid (\u0026ldquo;left edge\u0026rdquo;) \u0026ldquo;terminal\u0026rdquo; corner of the grid (\u0026ldquo;right edge\u0026rdquo;) dimensions along each axis if irregular, the cell-spacing along each axis  There are of course a handful of other attributes that might be useful (and which we can sometimes deduce) but these are the basics. If we imagine that each of these requires 64-bits per axis per value, a standard (regular) grid requires 576 bits, or 72 bytes. If we were storing the actual value locations, each would require 3 64-bit numbers \u0026ndash; which means that as soon as we were storing 3 of them, we would\n(Of course, one probably doesn\u0026rsquo;t need to store dimensions as 64 bits, and there are also probably some other ways to reduce the info necessary, but as straw-person arguments go, this isn\u0026rsquo;t so bad.)\nWhat we can get to with this is that for grid and other regular datasets, it\u0026rsquo;s reasonably cheap to index the data. So when we create a data object, for instance:\nsp = ds.sphere(\u0026quot;center\u0026quot;, (100.0, \u0026quot;kpc\u0026quot;))  yt can determine without touching the disk how many grid cells intersect it, and thus it can pre-allocate arrays of the correct size and fill them in progressively, in whatever fashion it deems best for IO purposes.\nThis isn\u0026rsquo;t without cost \u0026ndash; computing the intersections can be quite costly, and so we do some things to cache those. (The cost/benefit of caching often bites us when we are dealing with large unigrid datasets, though.) This was all designed to prevent having to call a big np.concatenate at some point in the operation when chunking based on \u0026quot;all\u0026quot;, but it\u0026rsquo;s not always obvious to me that the balance was correctly struck here.\nWhen an object is created, no selection is conducted until a field is requested. At some point in the call stack once a field is asked for, the function index._identify_base_chunk is called. This is where things are different for particles, but we\u0026rsquo;ll get to that later.\nParticle Indexing When dealing with particles, our indexing requirements are very different. Here, the cost of storing the index values is very high \u0026ndash; but, we also don\u0026rsquo;t want to have to perform too much IO. So we\u0026rsquo;re stuck minimizing how much IO we do, while also minimizing the amount of information we store in-memory once we \u0026ldquo;index\u0026rdquo; a dataset.\nIn yt-4.0, we accomplish this through the use of bitmap indices, which I described a little bit in the first post. The basic idea of this is that each \u0026ldquo;file\u0026rdquo; (which can be subsets of a single file, and is better thought of as an IO block of some type) is assigned some unique ID. All the files are iterated over and for each discrete point included in that file, an index into a space-filling curve is generated. We use a resonably coarse space filling curve for the first iteration \u0026ndash; say, a level 2 curve \u0026ndash; and that allows ambiguities. This is essentially a binning operation.\n(Incidentally, we often use Morton Z-Ordering because it\u0026rsquo;s just easier to explain. We might get better compression if we used Hilbert since consecutive values may be more likely to be identical.)\nAt the end of the first iteration, we have a key-value store of bitarrays, where the key is the file ID and the value is a set of 1\u0026rsquo;s and 0\u0026rsquo;s, where a 1 indicates that a particle is found in a given region identified by the space-filling curve index corresponding with that 1\u0026rsquo;s index in the array. So, for instance, if we had a level 3 index, we would have a set of bitarrays that looked like:\n001 000 101 010 011 011 ...  So, if we read from left-to-right, the first file has particles that live in (zero-indexed) indices 2, 6 and 8. The second file has particles in indices 1, 4, 5, 7 and 8.\nIf we know that our selector only intersects areas touched by index 2, then we only have to read from the first file.\nThis would work great if we had particles that were distributed pretty homogeneously on large scales, but in many cases, we don\u0026rsquo;t. Sometimes when particles are written to disk they are sorted on some high-order index and then written out in that order. What yt does is perform a secondary, as-needed indexing based on where there are \u0026ldquo;collisions\u0026rdquo; \u0026ndash; i.e., ambiguities. A set of logical operations is performed across all the bitarrays to identify where multiple files overlap; following this, a second round of indexing is conducted at a much higher spatial order.\nIn doing this, we are able to pinpoint with reasonably high precision the file or files that need to be read to get data from a given selector, and minimize very precisely the amount of over-reading that is done.\nUnfortunately, this doesn\u0026rsquo;t give us the ability to explicitly allocate arrays of the correct size. (And, the memory overhead of regaining that ability would be quite high.) But as we saw above, yt doesn\u0026rsquo;t want to do big concatenation operations! So it does the thing I really wish it didn\u0026rsquo;t, which is \u0026hellip; it reads all the position data in IO chunks, figures out how big it is (which only requires a running tally, not a set of allocated arrays), then allocates and fills that single big array.\nThis isn\u0026rsquo;t really that efficient, and it arises from the case where the indexing is comparatively cheap.\nBut all of this arises out of the design decision that we need to optimize for the case that we want a single big array, rather than a bunch of small arrays \u0026ndash; i.e., for the case of:\nds.r[:][\u0026quot;density\u0026quot;].max()  as opposed to\nds.r[:].max(\u0026quot;density\u0026quot;)  \u0026hellip;didn\u0026rsquo;t you say you\u0026rsquo;d be talking about Dask? Well, this is where dask comes in! And, it\u0026rsquo;s also why interfacing to dask is a bit tricky \u0026ndash; because we do a lot of work ahead of time before allocating any arrays, and then we get rid of the information generated during that work.\nIn an ideal world, what we would be able to do is to export a data object (such as a sphere or cylinder or rectangular prism) and a field-type (so we knew if it was a vector, or particles, or nodal/zonal data) as a dask array. For instance, if instead of returning an array (specifically, a YTArray or unyt_array) when we accessed sp[\u0026quot;density\u0026quot;], it returned a DaskArray, we would open up a number of new and interesting techniques.\nBut to do that, we need to be able to know in advance the chunk sizes, and more to the point, we need to be able to specify a function that returns each chunk uniquely.\nNext Entry: Iterables and IO Turns out, I thought I\u0026rsquo;d be done with this entry a lot sooner than I was!\nIn the next blog post, which hopefully will take less than the 8 days this one did, I\u0026rsquo;ll talk about why this is (currently) hard, how to fix that, and what we\u0026rsquo;re doing to fix it.\n","date":1560189573,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560189890,"objectID":"3be15376e926cc94641a4a597673f5ed","permalink":"https://matthewturk.github.io/post/refactoring-yt-frontends-part2/","publishdate":"2019-06-10T12:59:33-05:00","relpermalink":"/post/refactoring-yt-frontends-part2/","section":"post","summary":"SIDE NOTE: I intended for this blog post to be a bit shorter than it turned out, and for it to cover some things it \u0026hellip; didn\u0026rsquo;t! So it looks like there\u0026rsquo;ll be a part three in the series.\nOperations on Data Objects In my previous post, I walked through a few aspects of how the chunking system in yt works, mostly focusing on the \u0026quot;io\u0026quot; style of chunking, where the order in which data arrives is not important.","tags":[],"title":"Refactoring yt Frontends - Part 2","type":"post"},{"authors":[],"categories":null,"content":"","date":1559684783,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560367010,"objectID":"211427e115ea8ae0e1122db98bd34924","permalink":"https://matthewturk.github.io/talk/2019-06-04-odia-yt/","publishdate":"2019-06-04T14:46:23-07:00","relpermalink":"/talk/2019-06-04-odia-yt/","section":"talk","summary":"What have your experiences been with digital infrastructure development?","tags":[],"title":"yt: Hard Questions","type":"talk"},{"authors":[],"categories":[],"content":" In the still-in-development version of yt (4.0), the way that particles are handled has been redesigned from the ground up.\nThe current version of yt (3.x) utilizes an octree-based approach for meshing the particles, although not for indexing them \u0026ndash; which presents some problems when doing subsets of particles, as well as when doing visualizations that rely on an implicit meshing. The main result is that, in general, particle visualizations in yt 3.x aren\u0026rsquo;t that great, and are underresolved.\nIn yt 4.0, the particle system has been reimplemented to use EWAH bitmap indices (for more info, see Daniel Lemire\u0026rsquo;s EWAHBoolArray repository) to track which \u0026ldquo;regions\u0026rdquo; of files correspond to particular spatial regions, as designated by indices in a space-filling curve. Things are now orders of magnitude faster to load, to subset, and to visualize \u0026ndash; and the memory overhead is so much lower!\nThis work was led by Nathan Goldbaum and Meagan Lang, with crucial contributions from the rest of the yt community, including feedback and bugfixes from Bili Dong and Cameron Hummels.\nRecently, I\u0026rsquo;ve been exploring using a different array backend in yt, right now focusing on dask. While yt does lots of MPI-parallel operations, much of what we do with these has to be hand-programmed \u0026ndash; so when you implement a new DerivedQuantity (i.e., stuff like calling min on a data object) you have to jump through a few hoops related to intermediate values and the like. Plus, dask seems to be everywhere, and so if we exported to dask arrays or somehow interoperated better with it, we\u0026rsquo;d be able to interoperate with lots of the rest of the ecosystem more easily.\nUnfortunately, there\u0026rsquo;s a bit of an impedance mismatch which \u0026hellip; has made this more difficult than I\u0026rsquo;d like.\nReading Data Before getting too much further, though, I\u0026rsquo;m going to go through a bit about how yt thinks about \u0026ldquo;chunking\u0026rdquo; data.\nThe fundamental thing that yt does is index data. (Well, that, and take a while to compile all the Cython code.) Processing of the data is all layered on top of that \u0026ndash; including some pretty cool semantics-of-data and units, visualization, etc. The main thing is that if you do a subset, it knows where to go to grab that subset of data, and if you want to do something that touches everything, it\u0026rsquo;ll do its best to reduce the number of times data is loaded off disk in service of that.\nWe do this with a \u0026ldquo;chunking\u0026rdquo; system, which is implemented differently if your data is discrete (i.e., particles), mesh-based, and so on.\nSo to show what the problem is, I\u0026rsquo;m going to load up a dataset from the FIRE project.\nimport yt ds = yt.load(\u0026quot;data/FIRE_M12i_ref11/snapshot_600.hdf5\u0026quot;)  yt : [INFO ] 2019-06-02 16:02:22,303 Calculating time from 1.000e+00 to be 4.355e+17 seconds yt : [INFO ] 2019-06-02 16:02:22,304 Assuming length units are in kpc/h (comoving) yt : [INFO ] 2019-06-02 16:02:22,337 Parameters: current_time = 4.3545571088051386e+17 s yt : [INFO ] 2019-06-02 16:02:22,338 Parameters: domain_dimensions = [1 1 1] yt : [INFO ] 2019-06-02 16:02:22,339 Parameters: domain_left_edge = [0. 0. 0.] yt : [INFO ] 2019-06-02 16:02:22,341 Parameters: domain_right_edge = [60000. 60000. 60000.] yt : [INFO ] 2019-06-02 16:02:22,342 Parameters: cosmological_simulation = 1 yt : [INFO ] 2019-06-02 16:02:22,343 Parameters: current_redshift = 0.0 yt : [INFO ] 2019-06-02 16:02:22,344 Parameters: omega_lambda = 0.728 yt : [INFO ] 2019-06-02 16:02:22,344 Parameters: omega_matter = 0.272 yt : [INFO ] 2019-06-02 16:02:22,345 Parameters: omega_radiation = 0.0 yt : [INFO ] 2019-06-02 16:02:22,347 Parameters: hubble_constant = 0.702  At this point yt has done a tiny little bit of reading of the data \u0026ndash; just enough to figure out some of the metadata. It hasn\u0026rsquo;t indexed anything yet or read any of the actual data fields off of disk.\nNow let\u0026rsquo;s make a plot of the gas density, integrated over the z axis of the simulation. Keep in mind that in doing this, it will have to read all the gas particles and smooth them onto a buffer. The first time this gets run, an index is generated and then stored to disk. More on that in a moment.\nI\u0026rsquo;m going to use ds.r[:] here for \u0026ldquo;dataset region, but the whole thing\u0026rdquo; and then I call integrate on it and specify the field to integrate. Then, I plot it.\np=ds.r[:].integrate(\u0026quot;density\u0026quot;, axis=\u0026quot;z\u0026quot;).plot((\u0026quot;gas\u0026quot;, \u0026quot;density\u0026quot;))  yt : [INFO ] 2019-06-02 16:02:22,484 Allocating for 4.787e+06 particles Loading particle index: 100%|██████████| 10/10 [00:00\u0026lt;00:00, 817.25it/s] yt : [INFO ] 2019-06-02 16:02:23,623 xlim = 0.000000 60000.000000 yt : [INFO ] 2019-06-02 16:02:23,623 ylim = 0.000000 60000.000000 yt : [INFO ] 2019-06-02 16:02:23,633 Making a fixed resolution buffer of (('gas', 'density')) 800 by 800  (All that empty space is because there are only gas particles in the middle of the dataset!)\nThe first time any data needs to be read from a particle dataset, yt will construct an in-memory index of the data on disk; by default, it will store this in a sidecar file, so the next time that the dataset is read it does not need to be generated again.\nThe way the bitmap indices work is really fun, but that deserves its own blog post. It suffices to say that the indexing helps to figure out both which files to read, and which subsets of those files to read, since we don\u0026rsquo;t assume that the particles are sorted in any way. (Mostly because each code tends to sort the particles in its own way!)\nNow, for projecting over the whole domain, it\u0026rsquo;s not that big a deal to read everything, since we have to anyway, but if we did a subset it could dramatically reduce the IO necessary, and it also keeps much less data resident in memory than the old implementation.\nContinuing on, let\u0026rsquo;s say that we now want to center at a different location. We\u0026rsquo;d figure out the most dense point, and then set our center.\nc = ds.r[:].argmax((\u0026quot;gas\u0026quot;, \u0026quot;density\u0026quot;))  (One thing this next set of code highlights is that, in general, how we handle centers in yt is a bit clumsy at times. Writing this blog post led me to filing an issue which may or may not get any traction or support.)\np.set_origin(\u0026quot;center-window\u0026quot;) p.set_center((c[0], c[1])) p.zoom(25) p.set_zlim((\u0026quot;gas\u0026quot;,\u0026quot;density\u0026quot;), 1e-6, 1e-3)  yt : [INFO ] 2019-06-02 16:02:25,607 xlim = -713.911179 59286.088821 yt : [INFO ] 2019-06-02 16:02:25,611 ylim = 1049.283652 61049.283652 yt : [INFO ] 2019-06-02 16:02:25,619 Making a fixed resolution buffer of (('gas', 'density')) 800 by 800  So, we can visualize now, and it\u0026rsquo;s faster than it was before, and we also get much better results. Great. So why am I belaboring this point?\nIt\u0026rsquo;s because in the background, yt is queryin a data object to see which items to read off disk, then it is reading those items off disk. In this particular instance, it is doing what we call \u0026ldquo;io\u0026rdquo; chunking \u0026ndash; this means to use whatever type of hinting is best to get the most efficient ordering it knows how. Among other things, yt will try to minimize the number of times it opens a file, it seeks in a file, and it tries to keep the memory allocation count as low as possible.\n(I\u0026rsquo;ll write more on this last point later \u0026ndash; much of what yt does to index in yt-3.x and yt-4.0 is designed to keep the number of allocated arrays in the IO routines as low as possible, and to avoid any expensive concatenation or subselection operations. It turns out, this is \u0026hellip; not as big a deal as thought when this was made a design principle. And in general, it leads to a lot more floating point operations than we would like, and sometimes more stuff in memory, too.)\nAnd, so, uh, \u0026ldquo;chunking\u0026rdquo; is\u0026hellip;? We can figure out how yt chunks this data by, well, asking it to do it manually! Every data object presents a chunks interface which is a generator that modifies its internal state and then yields itself. For instance:\ndd = ds.all_data() for chunk in dd.chunks([], \u0026quot;io\u0026quot;): print(chunk[\u0026quot;particle_ones\u0026quot;].size)  1048576 885527 753678 524288 317696 262144 262144 262144 262144 208609  I mentioned that this generator yields itself; this is true. But the internal state is modified to store where we are in the iteration, along with things like the parameters for derived fields and the like. The source for this looks like this:\nfrom yt.data_objects.data_containers import YTSelectionContainer YTSelectionContainer.chunks??  Signature: YTSelectionContainer.chunks(self, fields, chunking_style, **kwargs) Docstring: \u0026lt;no docstring\u0026gt; Source: def chunks(self, fields, chunking_style, **kwargs): # This is an iterator that will yield the necessary chunks. self.get_data() # Ensure we have built ourselves if fields is None: fields = [] # chunk_ind can be supplied in the keyword arguments. If it's a # scalar, that'll be the only chunk that gets returned; if it's a list, # those are the ones that will be. chunk_ind = kwargs.pop(\u0026quot;chunk_ind\u0026quot;, None) if chunk_ind is not None: chunk_ind = ensure_list(chunk_ind) for ci, chunk in enumerate(self.index._chunk(self, chunking_style, **kwargs)): if chunk_ind is not None and ci not in chunk_ind: continue with self._chunked_read(chunk): self.get_data(fields) # NOTE: we yield before releasing the context yield self File: ~/yt/yt/yt/data_objects/data_containers.py Type: function  Note that this relies on the index object providing the _chunk routine, which interprets the type of chunking. Also, _chunked_read is a context manager which looks like this:\nYTSelectionContainer._chunked_read??  Signature: YTSelectionContainer._chunked_read(self, chunk) Docstring: \u0026lt;no docstring\u0026gt; Source: @contextmanager def _chunked_read(self, chunk): # There are several items that need to be swapped out # field_data, size, shape obj_field_data = [] if hasattr(chunk, 'objs'): for obj in chunk.objs: obj_field_data.append(obj.field_data) obj.field_data = YTFieldData() old_field_data, self.field_data = self.field_data, YTFieldData() old_chunk, self._current_chunk = self._current_chunk, chunk old_locked, self._locked = self._locked, False yield self.field_data = old_field_data self._current_chunk = old_chunk self._locked = old_locked if hasattr(chunk, 'objs'): for obj in chunk.objs: obj.field_data = obj_field_data.pop(0) File: ~/yt/yt/yt/data_objects/data_containers.py Type: function  This is a bit clunky, but it stores the old state (because, believe it or not, sometimes we have multiple levels of chunking simultaneously, especially for things like spatial derivatives) and then it makes a fresh state, and then it resets it after the context manager concludes.\nSo the end result here is that we have a mechanism that divides the dataset up into the chunks it needs (YTDataChunk objects), and then iterates over them. What does this look like for our particle dataset? Well, we can find out, evidently, by looking at the _current_chunk attribute on the object yielded by chunks.\nI\u0026rsquo;ve changed what we print out here just a little bit, because I want to keep the output a bit more human readable, but this is what it looks like:\ndd = ds.all_data() for chunk in dd.chunks([], \u0026quot;io\u0026quot;): print(\u0026quot;\\nExamining chunk...\u0026quot;) for obj in chunk._current_chunk.objs: print(\u0026quot; Examining obj...\u0026quot;,) for data_file in obj.data_files: print(\u0026quot; {}: {}-{}\u0026quot;.format(data_file.filename, data_file.start, data_file.end))  Examining chunk... Examining obj... /home/matthewturk/data/FIRE_M12i_ref11/snapshot_600.hdf5: 0-262144 Examining chunk... Examining obj... /home/matthewturk/data/FIRE_M12i_ref11/snapshot_600.hdf5: 262144-524288 Examining chunk... Examining obj... /home/matthewturk/data/FIRE_M12i_ref11/snapshot_600.hdf5: 524288-786432 Examining chunk... Examining obj... /home/matthewturk/data/FIRE_M12i_ref11/snapshot_600.hdf5: 786432-1048576 Examining chunk... Examining obj... /home/matthewturk/data/FIRE_M12i_ref11/snapshot_600.hdf5: 1048576-1310720 Examining chunk... Examining obj... /home/matthewturk/data/FIRE_M12i_ref11/snapshot_600.hdf5: 1310720-1572864 Examining chunk... Examining obj... /home/matthewturk/data/FIRE_M12i_ref11/snapshot_600.hdf5: 1572864-1835008 Examining chunk... Examining obj... /home/matthewturk/data/FIRE_M12i_ref11/snapshot_600.hdf5: 1835008-2097152 Examining chunk... Examining obj... /home/matthewturk/data/FIRE_M12i_ref11/snapshot_600.hdf5: 2097152-2359296 Examining chunk... Examining obj... /home/matthewturk/data/FIRE_M12i_ref11/snapshot_600.hdf5: 2359296-2567905  A few notes here. Each chunk is able to have multiple \u0026ldquo;objects\u0026rdquo; associated with it (which in grid frontends usually means multiple grid objects) but here, we have only one entry in the obj list associated with each. Each obj then only has one item in data_files, which is not really a data file, but instead a subset of a data file specified by its start and end indices.\nIf you\u0026rsquo;re thinking this is a bit clumsy, I would agree with you.\nDask Stuff The issue that I wrote about at the start of this blog post shows up when we start looking at how these chunks are generated. In principle, this does not map that badly to how dask expect chunks to be emitted.\n(At this point I need to admit that while I\u0026rsquo;ve worked with dask, it\u0026rsquo;s entirely possible that I am going to misrepresent its capabilities. Any errors are my own, and if I find out I am mistaken about any of this, I will happily update this blog post!)\nIt\u0026rsquo;s possible to create a dask array through the dask.array.Array constructor; this is described in the array design docs. Since yt uses unyt for attaching units we will need to do some additional work, but let\u0026rsquo;s imagine that we are simply happy dealing with unit-less (and, I suppose, unyt-less) arrays for now.\nTo generate these arrays most efficiently, we need to be able to specify their size, how to obtain them, and maybe a couple other things. But for our purposes, those are the two most important things.\nUnfortunately, as you might be able to tell, this is not information that is super easily exposed without iterating over the dataset. Sure, if we iterated and read everything, of course we can show the appropriate info. And, I posted a little bit about how one might do this on issue 1891, but there\u0026rsquo;s a key thing going on in that code \u0026ndash; yt has already read all the data from disk.\nSo, this isn\u0026rsquo;t ideal.\nChunks are not persistent This all comes about because chunks are not persistent, and more specifically, chunks are always create on-demand. Each different data object will have its own set of chunks, and these will map differently. So, for instance, we might end up selecting all the same sets of objects, but they will have different sizes (and even each different field might be a different size).\nsp1 = ds.sphere(c, (1, \u0026quot;Mpc\u0026quot;)) sp2 = ds.r[ (20.0, \u0026quot;Mpc\u0026quot;) : (40.0, \u0026quot;Mpc\u0026quot;), (25.0, \u0026quot;Mpc\u0026quot;) : (45.0, \u0026quot;Mpc\u0026quot;), (55.0, \u0026quot;Mpc\u0026quot;) : (65.0, \u0026quot;Mpc\u0026quot;) ] print(\u0026quot;sp1 len == {}\\nsp2 len == {}\u0026quot;.format( len(list(sp1.chunks([], \u0026quot;io\u0026quot;))), len(list(sp2.chunks([], \u0026quot;io\u0026quot;))) )) print(\u0026quot;sp1 =\u0026gt; \u0026quot;, \u0026quot; \u0026quot;.join(str(chunk[\u0026quot;particle_ones\u0026quot;].size) for chunk in sp1.chunks([], \u0026quot;io\u0026quot;))) print(\u0026quot;sp2 =\u0026gt; \u0026quot;, \u0026quot; \u0026quot;.join(str(chunk[\u0026quot;particle_ones\u0026quot;].size) for chunk in sp2.chunks([], \u0026quot;io\u0026quot;)))  sp1 len == 10 sp2 len == 10 sp1 =\u0026gt; 388571 306586 341808 205880 50260 2 1 2 3 0 sp2 =\u0026gt; 12 3673 480 29 146 200 77 419 3697 400  The trickiest part of this is that in these cases, we don\u0026rsquo;t know how big each one is going to be! For other types of indexing, it\u0026rsquo;s slightly different \u0026ndash; the indexing system for grids and octrees and meshes can figure out in advance (without reading data from disk) the precise number of values that will be read. But for particles we don\u0026rsquo;t necessarily know.\nUnfortunately, even if we did, the way that the YTDataChunk objects are the result of creating, then yield-ing, rather than returning a list of objects with known sizes makes it harder to expose this to dask. In particular, because we can\u0026rsquo;t (inexpensively) fast-forward the generator or rewind it or even access it elementwise makes it tricky to interface. One can expose unknown chunk sizes to dask, but it seems like we could do better.\nSo what can be done? Well, let me first note that a lot of this is a result of trying to be clever! Back when the chunking system was being implemented, it seemed like simple generator expressions were the right way to do it. And, a bunch of layers have been added on top of those generator expressions that make it harder to simply strip that component out.\nBut recently, Britton Smith and I have been digging into some of the particle frontends, and we think we might have a solution that would both simplify a lot of this logic and make it a lot easier to expose the arrays to different array backends \u0026ndash; specifically dask.\nFor more on that, wait for part two!\n","date":1559340372,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559509871,"objectID":"e2409a44c1f2fb08fb0b73f5ab5788e4","permalink":"https://matthewturk.github.io/post/refactoring-yt-frontends-part1/","publishdate":"2019-05-31T17:06:12-05:00","relpermalink":"/post/refactoring-yt-frontends-part1/","section":"post","summary":"The first post in a deep dive into yt frontends, chunking, and why and how they might be refactored.","tags":[],"title":"Refactoring yt Frontends - Part 1","type":"post"},{"authors":null,"categories":null,"content":"","date":1557177039,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559249134,"objectID":"2fd5b0f658881a3f851b03da682bc48d","permalink":"https://matthewturk.github.io/project/yt/","publishdate":"2019-05-06T16:10:39-05:00","relpermalink":"/project/yt/","section":"project","summary":"yt is an open-source python package for analyzing and visualizing volumetric data.","tags":[],"title":"yt","type":"project"},{"authors":[],"categories":null,"content":"","date":1556736856,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557177314,"objectID":"5f15fff70af789e7da9d4a78640dfa59","permalink":"https://matthewturk.github.io/talk/2019-05-01-crops-dependencies/","publishdate":"2019-05-01T13:54:16-05:00","relpermalink":"/talk/2019-05-01-crops-dependencies/","section":"talk","summary":"This is a brief overview of how we can think about dependencies in the Crops in Silico project, and how we can use that to organize our work and collaboration.","tags":[],"title":"Crops-in-Silico Collaboration and Dependencies","type":"talk"},{"authors":["Adam Brinckman","Kyle Chard","Niall Gaffney","Mihael Hategan","Matthew B Jones","Kacper Kowalik","Sivakumar Kulasekaran","Bertram Ludäscher","Bryce D Mecum","Jarek Nabrzyski","Victoria Stodden","Ian J Taylor","Matthew J Turk","Kandace Turner"],"categories":null,"content":"","date":1556668800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"4d610cc35353e551e4dd5786294bba6d","permalink":"https://matthewturk.github.io/publication/brinckman-2019-oa/","publishdate":"2019-05-30T20:07:09.390766Z","relpermalink":"/publication/brinckman-2019-oa/","section":"publication","summary":"The act of sharing scientific knowledge is rapidly evolving away from traditional articles and presentations to the delivery of executable objects that integrate the data and computational details (e.g., scripts and workflows) upon which the findings rely. This envisioned coupling of data and process is essential to advancing science but faces technical and institutional barriers. The Whole Tale project aims to address these barriers by connecting computational, data-intensive research efforts with the larger research process---transforming the knowledge discovery and dissemination process into one where data products are united with research articles to create ``living publications'' or tales. The Whole Tale focuses on the full spectrum of science, empowering users in the long tail of science, and power users with demands for access to big data and compute resources. We report here on the design, architecture, and implementation of the Whole Tale environment.","tags":["Living publications; Reproducibility; Provenance; Data sharing; Code sharing;Authorship"],"title":"Computing environments for reproducibility: Capturing the ``Whole Tale''","type":"publication"},{"authors":[],"categories":null,"content":"","date":1556082000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557177314,"objectID":"8e090a510726760f69cb7803cc7fcc0c","permalink":"https://matthewturk.github.io/talk/2019-04-24-ddd-update/","publishdate":"2019-05-01T13:52:48-05:00","relpermalink":"/talk/2019-04-24-ddd-update/","section":"talk","summary":"My 'update' talk at the Moore Data Driven Discovery Investigator Symposium in April, 2019.","tags":[],"title":"DDD Update","type":"talk"},{"authors":[],"categories":null,"content":"","date":1554058436,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557177314,"objectID":"3e44c17136c4c894688a3bcd35473a56","permalink":"https://matthewturk.github.io/talk/2019-03-31-troubleshooting-data-storytelling/","publishdate":"2019-05-01T13:53:56-05:00","relpermalink":"/talk/2019-03-31-troubleshooting-data-storytelling/","section":"talk","summary":"","tags":[],"title":"Troubleshooting Data Storytelling","type":"talk"},{"authors":[],"categories":null,"content":"","date":1553893200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557177314,"objectID":"b4dbdf946084cccc2be51a72048b1513","permalink":"https://matthewturk.github.io/talk/2019-03-29-cirss-open-source-teen-years/","publishdate":"2019-04-30T17:34:14-05:00","relpermalink":"/talk/2019-03-29-cirss-open-source-teen-years/","section":"talk","summary":"In this talk, I will reflect on experiences I have had navigating the landscape of open source scholarly software as projects age, and the way that shapes interaction in an ecosystem.  I will also report some recent developments with the open source project yt, and how they both interact with the shifting needs and desires of community members and how they address the needs and desires of potential future community members.  Many exciting buzzwords such as 'Rust' and 'WebAssembly' will be used.","tags":[],"title":"CIRSS Seminar: Open Source in the Teen Years","type":"talk"},{"authors":null,"categories":null,"content":"","date":1549052179,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559249134,"objectID":"4600419641aba970b03879da5587bed8","permalink":"https://matthewturk.github.io/project/crops-in-silico/","publishdate":"2019-02-01T15:16:19-05:00","relpermalink":"/project/crops-in-silico/","section":"project","summary":"Crops in Silico is an integrative and multi-scale modeling platform to combine modeling efforts toward the generation of virtual crops, open and accessible to the global community.","tags":[],"title":"Crops in Silico","type":"project"},{"authors":null,"categories":null,"content":"","date":1548879389,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559249134,"objectID":"670e9b7050687e20c063667daf003870","permalink":"https://matthewturk.github.io/project/whole-tale/","publishdate":"2019-01-30T15:16:29-05:00","relpermalink":"/project/whole-tale/","section":"project","summary":"Whole Tale is an initiative to build a scalable, open source, web-based, multi-user platform for reproducible research.","tags":[],"title":"Whole Tale","type":"project"},{"authors":null,"categories":null,"content":"This course covered topics that could broadly be described as \u0026ldquo;advanced,\u0026rdquo; including new platforms and tools for visualizing data, and how to present data in different, more thoughtful ways. It was structured differently than the other data viz courses, and designed for more interaction with a smaller group of students.\n","date":1546383114,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559268143,"objectID":"81e4db4ee0b05df2af767d7e3ba9a362","permalink":"https://matthewturk.github.io/courses/is590adv-spr2019/","publishdate":"2019-01-01T17:51:54-05:00","relpermalink":"/courses/is590adv-spr2019/","section":"courses","summary":"Seminar on advanced or in-depth topics in data visualization","tags":[],"title":"IS590ADV - Spring 2019","type":"courses"},{"authors":null,"categories":null,"content":"This course, offered in Fall of 2018, included more javascript than previous iterations and also utilized bqplot to a greater extent.\n","date":1533163907,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559268143,"objectID":"edece407444789734693d605192694b5","permalink":"https://matthewturk.github.io/courses/is590dv-fall2018/","publishdate":"2018-08-01T17:51:47-05:00","relpermalink":"/courses/is590dv-fall2018/","section":"courses","summary":"Data Viz from Fall 2018","tags":[],"title":"IS590DV - Fall 2018","type":"courses"},{"authors":["Nathan J Goldbaum","John A ZuHone","Matthew J Turk","Kacper Kowalik","Anna L Rosen"],"categories":null,"content":"","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"abc200f366401cf32068cf880c72c3bb","permalink":"https://matthewturk.github.io/publication/goldbaum-2018-ws/","publishdate":"2019-05-30T20:07:09.372097Z","relpermalink":"/publication/goldbaum-2018-ws/","section":"publication","summary":"Software that processes real-world data or that models a physical system must have some way of managing units. While simple approaches like the understood convention that all data are in a unit system (such as the MKS SI unit system) do work in practice, they are fraught with possible sources of error both by developers and users of the software. In this paper we present unyt, a Python library based on NumPy and SymPy for handling data that has units. It is designed both to aid quick interactive calculations and to be tightly integrated into a larger Python application or library. We compare unyt with two other Python libraries for handling units, Pint and astropy.units, and find that unyt is faster, has higher test coverage, and has fewer lines of code.","tags":["Authorship"],"title":"unyt: Handle, manipulate, and convert data with units in Python","type":"publication"},{"authors":null,"categories":null,"content":"This was an experimental course designed to convey the basics of \u0026ldquo;conversational computation\u0026rdquo; to astronomy undergraduates.\n","date":1514847094,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559268143,"objectID":"a66798ccee3cefb7f8c172b6f85651f8","permalink":"https://matthewturk.github.io/courses/astr496-spr2018/","publishdate":"2018-01-01T17:51:34-05:00","relpermalink":"/courses/astr496-spr2018/","section":"courses","summary":"Introduction to Computational Astrophysics","tags":[],"title":"ASTR496 - Spring 2018","type":"courses"},{"authors":null,"categories":null,"content":"Starting in Spring 2018, I transitioned courses to using Github Pages and RevealJS. The repository includes built slide decks and rendered Jupyter notebooks.\n","date":1514847088,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559268143,"objectID":"106a69791a1c8e84dba3b3f48bdfd470","permalink":"https://matthewturk.github.io/courses/is590dv-spr2018/","publishdate":"2018-01-01T17:51:28-05:00","relpermalink":"/courses/is590dv-spr2018/","section":"courses","summary":"Data Viz from Spring 2018","tags":[],"title":"IS590DV - Spring 2018","type":"courses"},{"authors":["Hsi-Yu Schive","John A ZuHone","Nathan J Goldbaum","Matthew J Turk","Massimo Gaspari","Chin-Yu Cheng"],"categories":null,"content":"","date":1512086400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"3b5789604fd9a3ad84d80b2d4e03c5fe","permalink":"https://matthewturk.github.io/publication/schive-2017-ep/","publishdate":"2019-05-30T20:07:09.39481Z","relpermalink":"/publication/schive-2017-ep/","section":"publication","summary":"We present GAMER-2, a GPU-accelerated adaptive mesh refinement (AMR) code for astrophysics. It provides a rich set of features, including adaptive time-stepping, several hydrodynamic schemes, magnetohydrodynamics, self-gravity, particles, star formation, chemistry and radiative processes with GRACKLE, data analysis with yt, and memory pool for efficient object allocation. GAMER-2 is fully bitwise reproducible. For the performance optimization, it adopts hybrid OpenMP/MPI/GPU parallelization and utilizes overlapping CPU computation, GPU computation, and CPU-GPU communication. Load balancing is achieved using a Hilbert space-filling curve on a level-by-level basis without the need to duplicate the entire AMR hierarchy on each MPI process. To provide convincing demonstrations of the accuracy and performance of GAMER-2, we directly compare with Enzo on isolated disk galaxy simulations and with FLASH on galaxy cluster merger simulations. We show that the physical results obtained by different codes are in very good agreement, and GAMER-2 outperforms Enzo and FLASH by nearly one and two orders of magnitude, respectively, on the Blue Waters supercomputers using $1-256$ nodes. More importantly, GAMER-2 exhibits similar or even better parallel scalability compared to the other two codes. We also demonstrate good weak and strong scaling using up to 4096 GPUs and 65,536 CPU cores, and achieve a uniform resolution as high as $10,240^3$ cells. Furthermore, GAMER-2 can be adopted as an AMR+GPUs framework and has been extensively used for the wave dark matter ($ψ$DM) simulations. GAMER-2 is open source (available at https://github.com/gamer-project/gamer) and new contributions are welcome.","tags":["Authorship;DXL Member Papers"],"title":"GAMER-2: a GPU-accelerated adaptive mesh refinement code -- accuracy, performance, and scalability","type":"publication"},{"authors":null,"categories":null,"content":"This was the second semester that I taught Data Viz, and I was still largely using Google Slides. The link to the repository includes the lecture PDFs and notebooks used.\n","date":1501627863,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559268143,"objectID":"2882197390cd2fb552718a0c7b83241b","permalink":"https://matthewturk.github.io/courses/lis590dv-fall2017/","publishdate":"2017-08-01T17:51:03-05:00","relpermalink":"/courses/lis590dv-fall2017/","section":"courses","summary":"Data Viz from Fall 2017","tags":[],"title":"LIS590DV - Fall 2017","type":"courses"},{"authors":["Amy Marshall-Colon","Stephen P Long","Douglas K Allen","Gabrielle Allen","Daniel A Beard","Bedrich Benes","Susanne von Caemmerer","A J Christensen","Donna J Cox","John C Hart","Peter M Hirst","Kavya Kannan","Daniel S Katz","Jonathan P Lynch","Andrew J Millar","Balaji Panneerselvam","Nathan D Price","Przemyslaw Prusinkiewicz","David Raila","Rachel G Shekar","Stuti Shrivastava","Diwakar Shukla","Venkatraman Srinivasan","Mark Stitt","Matthew J Turk","Eberhard O Voit","Yu Wang","Xinyou Yin","Xin-Guang Zhu"],"categories":null,"content":"","date":1493596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"bb0989fb716998e413388479a9bd5b8e","permalink":"https://matthewturk.github.io/publication/marshall-colon-2017-fb/","publishdate":"2019-05-30T20:07:09.377714Z","relpermalink":"/publication/marshall-colon-2017-fb/","section":"publication","summary":"Multi-scale models can facilitate whole plant simulations by linking gene networks, protein synthesis, metabolic pathways, physiology, and growth. Whole plant models can be further integrated with ecosystem, weather, and climate models to predict how various interactions respond to environmental perturbations. These models have the potential to fill in missing mechanistic details and generate new hypotheses to prioritize directed engineering efforts. Outcomes will potentially accelerate improvement of crop yield, sustainability, and increase future food security. It is time for a paradigm shift in plant modeling, from largely isolated efforts to a connected community that takes advantage of advances in high performance computing and mechanistic understanding of plant processes. Tools for guiding future crop breeding and engineering, understanding the implications of discoveries at the molecular level for whole plant behavior, and improved prediction of plant and ecosystem responses to the environment are urgently needed. The purpose of this perspective is to introduce Crops in silico (cropsinsilico.org), an integrative and multi-scale modeling platform, as one solution that combines isolated modeling efforts toward the generation of virtual crops, which is open and accessible to the entire plant biology community. The major challenges involved both in the development and deployment of a shared, multi-scale modeling platform, which are summarized in this prospectus, were recently identified during the first Crops in silico Symposium and Workshop.","tags":["computational framework; crop yield; integration; model; multiscale;Authorship"],"title":"Crops In Silico: Generating Virtual Crops Using an Integrative and Multi-scale Modeling Platform","type":"publication"},{"authors":["Britton D Smith","Greg L Bryan","Simon C O Glover","Nathan J Goldbaum","Matthew J Turk","John Regan","John H Wise","Hsi-Yu Schive","Tom Abel","Andrew Emerick","Brian W O'Shea","Peter Anninos","Cameron B Hummels","Sadegh Khochfar"],"categories":null,"content":"","date":1491004800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"01c869b2c02e627571168f1e7ea367d2","permalink":"https://matthewturk.github.io/publication/smith-2017-za/","publishdate":"2019-05-30T20:07:09.393935Z","relpermalink":"/publication/smith-2017-za/","section":"publication","summary":"We present the grackle chemistry and cooling library for astrophysical simulations and models. grackle provides a treatment of non-equilibrium primordial chemistry and cooling for H, D and He species, including H2 formation on dust grains; tabulated primordial and metal cooling; multiple ultraviolet background models; and support for radiation transfer and arbitrary heat sources. The library has an easily implementable interface for simulation codes written in c, c++ and fortran as well as a python interface with added convenience functions for semi-analytical models. As an open-source project, grackle provides a community resource for accessing and disseminating astrochemical data and numerical methods. We present the full details of the core functionality, the simulation and python interfaces, testing infrastructure, performance and range of applicability. grackle is a fully open-source project and new contributions are welcome.","tags":["Authorship;DXL Member Papers"],"title":"grackle: a chemistry and cooling library for astrophysics","type":"publication"},{"authors":null,"categories":null,"content":"This was the first time I taught Data Viz, and the course repository includes the original Google Slides PDFs, links to the presentations, and the notebooks used in the course.\n","date":1483246801,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559268143,"objectID":"a8ae1d547cdc224350917d8d2bd536a0","permalink":"https://matthewturk.github.io/courses/lis590dv-spr2017/","publishdate":"2017-01-01T00:00:01-05:00","relpermalink":"/courses/lis590dv-spr2017/","section":"courses","summary":"Data Viz from Spring 2017","tags":[],"title":"LIS590DV - Spring 2017","type":"courses"},{"authors":["Harshil M Kamdar","Matthew J Turk","Robert J Brunner"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"d2de67e520898676ea8c2ae39f8eaf22","permalink":"https://matthewturk.github.io/publication/kamdar-2016-ae/","publishdate":"2019-05-30T20:07:09.37875Z","relpermalink":"/publication/kamdar-2016-ae/","section":"publication","summary":"We present a new exploratory framework to model galaxy formation and evolution in a hierarchical Universe by using machine learning (ML). Our motivations are two-fold: (1) presenting a new, promising technique to study galaxy formation, and (2) quantitatively analysing the extent of the influence of dark matter halo properties on galaxies in the backdrop of semi-analytical models (SAMs). We use the influential Millennium Simulation and the corresponding Munich SAM to train and test various sophisticated ML algorithms (k-Nearest Neighbors, decision trees, random forests, and extremely randomized trees). By using only essential dark matter halo physical properties for haloes of M  1012 M⊙ and a partial merger tree, our model predicts the hot gas mass, cold gas mass, bulge mass, total stellar mass, black hole mass and cooling radius at z = 0 for each central galaxy in a dark matter halo for the Millennium run. Our results provide a unique and powerful phenomenological framework to explore the galaxy--halo connection that is built upon SAMs and demonstrably place ML as a promising and a computationally efficient tool to study small-scale structure formation.","tags":["Authorship"],"title":"Machine learning and cosmological simulations -- I. Semi-analytical models","type":"publication"},{"authors":["Harshil M Kamdar","Matthew J Turk","Robert J Brunner"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"0e563eede49bd354c26f1fd53f232568","permalink":"https://matthewturk.github.io/publication/kamdar-2016-mu/","publishdate":"2019-05-30T20:07:09.376485Z","relpermalink":"/publication/kamdar-2016-mu/","section":"publication","summary":"We extend a machine learning (ML) framework presented previously to model galaxy formation and evolution in a hierarchical universe using N-body+ hydrodynamical simulations. In this work, we show that ML is a promising technique to study galaxy formation …","tags":["Authorship"],"title":"Machine learning and cosmological simulations--II. Hydrodynamical simulations","type":"publication"},{"authors":["Desika Narayanan","Matthew Turk","Robert Feldmann","Thomas Robitaille","Philip Hopkins","Robert Thompson","Christopher Hayward","David Ball","Claude-André Faucher-Giguère","Dušan Kereš"],"categories":null,"content":"","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"f598bdf0862f455746a898a59f677c1a","permalink":"https://matthewturk.github.io/publication/narayanan-2015-gq/","publishdate":"2019-05-30T20:07:09.392775Z","relpermalink":"/publication/narayanan-2015-gq/","section":"publication","summary":"Submillimetre-bright galaxies at high redshift are the most luminous, heavily star-forming galaxies in the Universe and are characterized by prodigious emission in the far-infrared, with a flux of at least five millijanskys at a wavelength of 850 micrometres. They reside in haloes with masses about 10(13) times that of the Sun, have low gas fractions compared to main-sequence disks at a comparable redshift, trace complex environments and are not easily observable at optical wavelengths. Their physical origin remains unclear. Simulations have been able to form galaxies with the requisite luminosities, but have otherwise been unable to simultaneously match the stellar masses, star formation rates, gas fractions and environments. Here we report a cosmological hydrodynamic galaxy formation simulation that is able to form a submillimetre galaxy that simultaneously satisfies the broad range of observed physical constraints. We find that groups of galaxies residing in massive dark matter haloes have increasing rates of star formation that peak at collective rates of about 500-1,000 solar masses per year at redshifts of two to three, by which time the interstellar medium is sufficiently enriched with metals that the region may be observed as a submillimetre-selected system. The intense star formation rates are fuelled in part by the infall of a reservoir gas supply enabled by stellar feedback at earlier times, not through major mergers. With a lifetime of nearly a billion years, our simulations show that the submillimetre-bright phase of high-redshift galaxies is prolonged and associated with significant mass buildup in early-Universe proto-clusters, and that many submillimetre-bright galaxies are composed of numerous unresolved components (for which there is some observational evidence).","tags":["Authorship;DXL Member Papers"],"title":"The formation of submillimetre-bright galaxies from gas infall over a billion years","type":"publication"},{"authors":["M Turk"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"96c4421b91ab99587be7a001045f86a2","permalink":"https://matthewturk.github.io/publication/turk-2015-yw/","publishdate":"2019-05-30T20:07:09.373042Z","relpermalink":"/publication/turk-2015-yw/","section":"publication","summary":"Gmail. It serves as the center of my digital life. When I receive a new email with a Google Docs document linked in it, there's a preview of that document--- when I send one to someone else, Gmail first checks to see if the person receiving it has permission to view it and lets me know …","tags":["Authorship"],"title":"Vertical Integration","type":"publication"},{"authors":["Samuel W Skillman","Michael S Warren","Matthew J Turk","Risa H Wechsler","Daniel E Holz","P M Sutter"],"categories":null,"content":"","date":1404172800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"c4612415f473e20e2680d86b0e53ec81","permalink":"https://matthewturk.github.io/publication/skillman-2014-vt/","publishdate":"2019-05-30T20:07:09.380327Z","relpermalink":"/publication/skillman-2014-vt/","section":"publication","summary":"The Dark Sky Simulations are an ongoing series of cosmological N-body simulations designed to provide a quantitative and accessible model of the evolution of the large-scale Universe. Such models are essential for many aspects of the study of dark matter and dark energy, since we lack a sufficiently accurate analytic model of non-linear gravitational clustering. In July 2014, we made available to the general community our early data release, consisting of over 55 Terabytes of simulation data products, including our largest simulation to date, which used $1.07 times 10^12~(10240^3)$ particles in a volume $8h^-1mathrmGpc$ across. Our simulations were performed with 2HOT, a purely tree-based adaptive N-body method, running on 200,000 processors of the Titan supercomputer, with data analysis enabled by yt. We provide an overview of the derived halo catalogs, mass function, power spectra and light cone data. We show self-consistency in the mass function and mass power spectrum at the 1% level over a range of more than 1000 in particle mass. We also present a novel method to distribute and access very large datasets, based on an abstraction of the World Wide Web (WWW) as a file system, remote memory-mapped file access semantics, and a space-filling curve index. This method has been implemented for our data release, and provides a means to not only query stored results such as halo catalogs, but also to design and deploy new analysis techniques on large distributed datasets.","tags":["Authorship"],"title":"Dark Sky Simulations: Early Data Release","type":"publication"},{"authors":["M Turk"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"a1f14428b663d7f3b82f91bbfe0a9609","permalink":"https://matthewturk.github.io/publication/turk-2014-ak/","publishdate":"2019-05-30T20:07:09.364588Z","relpermalink":"/publication/turk-2014-ak/","section":"publication","summary":"Designing Software for Collaboration In computational science, collaboration in different scientific communities often takes different forms---examining the results of data generation or acquisition, combining dispa- rate pieces of software in a computational pipeline, and even enhancing …","tags":["Authorship"],"title":"Fostering Collaborative Computational Science","type":"publication"},{"authors":["Benjamin Holtzman","Jason Candler","Matthew Turk","Daniel Peter"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"ce1db4fcedcc809c4ff9dcee5ab6f2ff","permalink":"https://matthewturk.github.io/publication/holtzman-2014-wb/","publishdate":"2019-05-30T20:07:09.373678Z","relpermalink":"/publication/holtzman-2014-wb/","section":"publication","summary":"We construct a representation of earthquakes and global seismic waves through sound and animated images. The seismic wave field is the ensemble of elastic waves that propagate through the planet after an earthquake, emanating from the rupture on the fault. The sounds are made by time compression (i.e. speeding up) of seismic data with minimal additional processing. The animated images are renderings of numerical simulations of seismic wave propagation in the globe. Synchronized sounds and images reveal complex patterns and illustrate numerous aspects of the seismic wave field. These movies represent phenomena occurring far from the time and length scales normally accessible to us, creating a profound experience for the observer. The multi-sensory perception of these complex phenomena may also bring new insights to researchers.","tags":["Authorship"],"title":"Seismic Sound Lab: Sights, Sounds and Perception of the Earth as an Acoustic Space","type":"publication"},{"authors":["T Kwasnitschka","K C Yu","M Turk"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"36670fa758f1f2a192e24ec7cefa9258","permalink":"https://matthewturk.github.io/publication/kwasnitschka-2014-ya/","publishdate":"2019-05-30T20:07:09.361875Z","relpermalink":"/publication/kwasnitschka-2014-ya/","section":"publication","summary":"What topics belong inside a planetarium and what is beyond the scope of our mission? Are we limited to astronomy?","tags":["Authorship"],"title":"The Ground beath our Feet: Earth Science in the Planetarium","type":"publication"},{"authors":["Ji-Hoon Kim","Mark R Krumholz","John H Wise","Matthew J Turk","Nathan J Goldbaum","Tom Abel"],"categories":null,"content":"","date":1383264000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"7dc7f1a8d9fa48cd9148edec88aee4a6","permalink":"https://matthewturk.github.io/publication/kim-2013-ac/","publishdate":"2019-05-30T20:07:09.371064Z","relpermalink":"/publication/kim-2013-ac/","section":"publication","summary":"We investigate the spatially resolved star formation relation using a galactic disk formed in a comprehensive high-resolution (3.8 pc) simulation. Our new implementation of stellar feedback includes ionizing radiation as well as supernova explosions, and we handle ionizing radiation by solving the radiative transfer equation rather than by a subgrid model. Photoheating by stellar radiation stabilizes gas against Jeans fragmentation, reducing the star formation rate (SFR). Because we have self-consistently calculated the location of ionized gas, we are able to make simulated, spatially resolved observations of star formation tracers, such as H$α$ emission. We can also observe how stellar feedback manifests itself in the correlation between ionized and molecular gas. Applying our techniques to the disk in a galactic halo of 2.3 $times$ 1011 M ☉, we find that the correlation between SFR density (estimated from mock H$α$ emission) and H2 density shows large scatter, especially at high resolutions of ≲75 pc that are comparable to the size of giant molecular clouds (GMCs). This is because an aperture of GMC size captures only particular stages of GMC evolution and because H$α$ traces hot gas around star-forming regions and is displaced from the H2 peaks themselves. By examining the evolving environment around star clusters, we speculate that the breakdown of the traditional star formation laws of the Kennicutt-Schmidt type at small scales is further aided by a combination of stars drifting from their birthplaces and molecular clouds being dispersed via stellar feedback.","tags":["Authorship"],"title":"DWARF GALAXIES WITH IONIZING RADIATION FEEDBACK. II. SPATIALLY RESOLVED STAR FORMATION RELATION","type":"publication"},{"authors":["Ji-Hoon Kim","Mark R Krumholz","John H Wise","Matthew J Turk","Nathan J Goldbaum","Tom Abel"],"categories":null,"content":"","date":1377993600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"d89d1130804c0344ac02ec8f41dc7d61","permalink":"https://matthewturk.github.io/publication/kim-2013-jy/","publishdate":"2019-05-30T20:07:09.379476Z","relpermalink":"/publication/kim-2013-jy/","section":"publication","summary":"We describe a new method for simulating ionizing radiation and supernova feedback in the analogs of low-redshift galactic disks. In this method, which we call star-forming molecular cloud (SFMC) particles, we use a ray-tracing technique to solve the radiative transfer equation for ultraviolet photons emitted by thousands of distinct particles on the fly. Joined with high numerical resolution of 3.8 pc, the realistic description of stellar feedback helps to self-regulate star formation. This new feedback scheme also enables us to study the escape of ionizing photons from star-forming clumps and from a galaxy, and to examine the evolving environment of star-forming gas clumps. By simulating a galactic disk in a halo of 2.3 $times$ 1011 M ☉, we find that the average escape fraction from all radiating sources on the spiral arms (excluding the central 2.5 kpc) fluctuates between 0.08% and 5.9% during a ∼20 Myr period with a mean value of 1.1%. The flux of escaped photons from these sources is not strongly beamed, but manifests a large opening angle of more than 60° from the galactic pole. Further, we investigate the escape fraction per SFMC particle, f esc(i), and how it evolves as the particle ages. We discover that the average escape fraction f esc is dominated by a small number of SFMC particles with high f esc(i). On average, the escape fraction from an SFMC particle rises from 0.27% at its birth to 2.1% at the end of a particle lifetime, 6 Myr. This is because SFMC particles drift away from the dense gas clumps in which they were born, and because the gas around the star-forming clumps is dispersed by ionizing radiation and supernova feedback. The framework established in this study brings deeper insight into the physics of photon escape fraction from an individual star-forming clump and from a galactic disk.","tags":["Authorship"],"title":"DWARF GALAXIES WITH IONIZING RADIATION FEEDBACK. I. ESCAPE OF IONIZING PHOTONS","type":"publication"},{"authors":["The Enzo Collaboration","Greg L Bryan","Michael L Norman","Brian W O'Shea","Tom Abel","John H Wise","Matthew J Turk","Daniel R Reynolds","David C Collins","Peng Wang","Samuel W Skillman","Britton Smith","Robert P Harkness","James Bordner","Ji-Hoon Kim","Michael Kuhlen","Hao Xu","Nathan Goldbaum","Cameron Hummels","Alexei G Kritsuk","Elizabeth Tasker","Stephen Skory","Christine M Simpson","Oliver Hahn","Jeffrey S Oishi","Geoffrey C So","Fen Zhao","Renyue Cen","Yuan Li"],"categories":null,"content":"","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"8517285c643a11800825d1dc3fe6aa6b","permalink":"https://matthewturk.github.io/publication/the-enzo-collaboration-2013-es/","publishdate":"2019-05-30T20:07:09.391823Z","relpermalink":"/publication/the-enzo-collaboration-2013-es/","section":"publication","summary":"This paper describes the open-source code Enzo, which uses block-structured adaptive mesh refinement to provide high spatial and temporal resolution for modeling astrophysical fluid flows. The code is Cartesian, can be run in 1, 2, and 3 dimensions, and supports a wide variety of physics including hydrodynamics, ideal and non-ideal magnetohydrodynamics, N-body dynamics (and, more broadly, self-gravity of fluids and particles), primordial gas chemistry, optically-thin radiative cooling of primordial and metal-enriched plasmas (as well as some optically-thick cooling models), radiation transport, cosmological expansion, and models for star formation and feedback in a cosmological context. In addition to explaining the algorithms implemented, we present solutions for a wide range of test problems, demonstrate the code's parallel performance, and discuss the Enzo collaboration's code development methodology.","tags":["Authorship"],"title":"Enzo: An Adaptive Mesh Refinement Code for Astrophysics","type":"publication"},{"authors":["M J Turk"],"categories":null,"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"c8a33d90212b796c8d03b80111c09aa0","permalink":"https://matthewturk.github.io/publication/turk-2013-ks/","publishdate":"2019-05-30T20:07:09.374781Z","relpermalink":"/publication/turk-2013-ks/","section":"publication","summary":"As scientists' needs for computational techniques and tools grow, they cease to be supportable by software developed in isolation. In many cases, these needs are being met by communities of practice, where software is developed by domain scientists to reach …","tags":["Authorship"],"title":"How to scale a code in the human dimension","type":"publication"},{"authors":["Matthew J Turk"],"categories":null,"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"2e693ceb78643251252efd3fdb648e1b","permalink":"https://matthewturk.github.io/publication/turk-2013-jj/","publishdate":"2019-05-30T20:07:09.375168Z","relpermalink":"/publication/turk-2013-jj/","section":"publication","summary":"As scientists' needs for computational techniques and tools grow, they cease to be supportable by software developed in isolation. In many cases, these needs are being met by communities of practice, where software is developed by domain scientists to reach …","tags":["XSEDE proceedings","community","open source;Authorship"],"title":"Scaling a Code in the Human Dimension","type":"publication"},{"authors":["John H Wise","Tom Abel","Matthew J Turk","Michael L Norman","Britton D Smith"],"categories":null,"content":"","date":1346457600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"52da72f3fa7de9f835f555ba2e7cb3a4","permalink":"https://matthewturk.github.io/publication/wise-2012-kl/","publishdate":"2019-05-30T20:07:09.362758Z","relpermalink":"/publication/wise-2012-kl/","section":"publication","summary":"Here we present three adaptive mesh refinement radiation hydrodynamics simulations that illustrate the impact of momentum transfer from ionising radiation to the absorbing gas on star formation in high-redshift dwarf galaxies. Momentum transfer is calculated by solving the radiative transfer equation with a ray tracing algorithm that is adaptive in spatial and angular coordinates. We find that momentum input partially affects star formation by increasing the turbulent support to a three-dimensional rms velocity equal to the circular velocity of early haloes. Compared to a calculation that neglects radiation pressure, the star formation rate is decreased by a factor of five to 1.8 ? 10?2 M? yr?1 in a dwarf galaxy with a dark matter and stellar mass of 2.0 ? 108 M? and 4.5 ? 105 M?, respectively, when radiation pressure is included. Its mean metallicity of 10?2.1 Z? is consistent with the observed dwarf galaxy luminosity-metallicity relation. In addition to photo-heating in H II regions, radiation pressure further drives dense gas from star forming regions, so supernovae feedback occurs in a warmer and more diffuse medium, launching metal-rich outflows.","tags":["Authorship"],"title":"The imprint of pop III stars on the first galaxies","type":"publication"},{"authors":["Matthew J Turk","Jeffrey S Oishi","Tom Abel","Greg L Bryan"],"categories":null,"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"3cbbedf0bd49e08ae8ed40e5800104c8","permalink":"https://matthewturk.github.io/publication/turk-2012-ac/","publishdate":"2019-05-30T20:07:09.382091Z","relpermalink":"/publication/turk-2012-ac/","section":"publication","summary":"We study the buildup of magnetic fields during the formation of Population III star-forming regions by conducting cosmological simulations from realistic initial conditions and varying the Jeans resolution. To investigate this in detail, we start simulations from identical initial conditions, mandating 16, 32, and 64 zones per Jeans length, and study the variation in their magnetic field amplification. We find that, while compression results in some amplification, turbulent velocity fluctuations driven by the collapse can further amplify an initially weak seed field via dynamo action, provided there is sufficient numerical resolution to capture vortical motions (we find this requirement to be 64 zones per Jeans length, slightly larger than but consistent with previous work run with more idealized collapse scenarios). We explore saturation of amplification of the magnetic field, which could potentially become dynamically important in subsequent, fully resolved calculations. We have also identified a relatively surprising phenomenon that is purely hydrodynamic: the higher-resolved simulations possess substantially different characteristics, including higher infall velocity, increased temperatures inside 1000 AU, and decreased molecular hydrogen content in the innermost region. Furthermore, we find that disk formation is suppressed in higher-resolution calculations, at least at the times that we can follow the calculation. We discuss the effect this may have on the buildup of disks over the accretion history of the first clump to form as well as the potential for gravitational instabilities to develop and induce fragmentation.","tags":["Authorship"],"title":"MAGNETIC FIELDS IN POPULATION III STAR FORMATION","type":"publication"},{"authors":["John H Wise","Tom Abel","Matthew J Turk","Michael L Norman","Britton D Smith"],"categories":null,"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"aa120229066e2ea9fc4e86004a779463","permalink":"https://matthewturk.github.io/publication/wise-2012-dn/","publishdate":"2019-05-30T20:07:09.382982Z","relpermalink":"/publication/wise-2012-dn/","section":"publication","summary":"Massive stars provide feedback that shapes the interstellar medium of galaxies at all redshifts and their resulting stellar populations. Here we present three adaptive mesh refinement radiation hydrodynamics simulations that illustrate the impact of momentum …","tags":["Authorship"],"title":"The birth of a galaxy--II. The role of radiation pressure","type":"publication"},{"authors":["John H Wise","Matthew J Turk","Michael L Norman","Tom Abel"],"categories":null,"content":"","date":1322697600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"2e513dfacd2ecb326d361c9ba89cffb2","permalink":"https://matthewturk.github.io/publication/wise-2011-ph/","publishdate":"2019-05-30T20:07:09.386569Z","relpermalink":"/publication/wise-2011-ph/","section":"publication","summary":"By definition, Population III stars are metal-free, and their protostellar collapse is driven by molecular hydrogen cooling in the gas phase, leading to large characteristic masses. Population II stars with lower characteristic masses form when the star-forming gas reaches a critical metallicity of 10--6-10--3.5 Z ☉. We present an adaptive mesh refinement radiation hydrodynamics simulation that follows the transition from Population III to Population II star formation. The maximum spatial resolution of 1 comoving parsec allows for individual molecular clouds to be well resolved and their stellar associations to be studied in detail. We model stellar radiative feedback with adaptive ray tracing. A top-heavy initial mass function for the Population III stars is considered, resulting in a plausible distribution of pair-instability supernovae and associated metal enrichment. We find that the gas fraction recovers from 5% to nearly the cosmic fraction in halos with merger histories rich in halos above 107 M ☉. A single pair-instability supernova is sufficient to enrich the host halo to a metallicity floor of 10--3 Z ☉ and to transition to Population II star formation. This provides a natural explanation for the observed floor on damped Ly$α$ systems metallicities reported in the literature, which is of this order. We find that stellar metallicities do not necessarily trace stellar ages, as mergers of halos with established stellar populations can create superpositions of t--Z evolutionary tracks. A bimodal metallicity distribution is created after a starburst occurs when the halo can cool efficiently through atomic line cooling.","tags":["Authorship"],"title":"THE BIRTH OF A GALAXY: PRIMORDIAL METAL ENRICHMENT AND STELLAR POPULATIONS","type":"publication"},{"authors":["M J Turk","B D Smith"],"categories":null,"content":"","date":1293840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"b3764bc9a12880080515cba2186c14bc","permalink":"https://matthewturk.github.io/publication/turk-2011-ck/","publishdate":"2019-05-30T20:07:09.366733Z","relpermalink":"/publication/turk-2011-ck/","section":"publication","summary":"The usage of the high-level scripting language Python has enabled new mechanisms for data interrogation, discovery and visualization of scientific data. We present yt, an open source, community-developed astrophysical analysis and visualization toolkit for data …","tags":["Authorship"],"title":"High-Performance Astrophysical Simulations and Analysis with Python","type":"publication"},{"authors":["Matthew J Turk","Paul Clark","S C O Glover","T H Greif","Tom Abel","Ralf Klessen","Volker Bromm"],"categories":null,"content":"","date":1291161600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"972f8cfd264acbc97e71613f0de8f131","permalink":"https://matthewturk.github.io/publication/turk-2010-mg/","publishdate":"2019-05-30T20:07:09.381233Z","relpermalink":"/publication/turk-2010-mg/","section":"publication","summary":"The transformation of atomic hydrogen to molecular hydrogen through three-body reactions is a crucial stage in the collapse of primordial, metal-free halos, where the first generation of stars (Population III stars) in the universe is formed. However, in the published literature, the rate coefficient for this reaction is uncertain by nearly an order of magnitude. We report on the results of both adaptive mesh refinement and smoothed particle hydrodynamics simulations of the collapse of metal-free halos as a function of the value of this rate coefficient. For each simulation method, we have simulated a single halo three times, using three different values of the rate coefficient. We find that while variation between halo realizations may be greater than that caused by the three-body rate coefficient being used, both the accretion physics onto Population III protostars as well as the long-term stability of the disk and any potential fragmentation may depend strongly on this rate coefficient.","tags":["Authorship"],"title":"EFFECTS OF VARYING THE THREE-BODY MOLECULAR HYDROGEN FORMATION RATE IN PRIMORDIAL STAR FORMATION","type":"publication"},{"authors":["Matthew J Turk","Michael L Norman","Tom Abel"],"categories":null,"content":"","date":1291161600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"ba4d0d7e4ffe9e250803df6633aca598","permalink":"https://matthewturk.github.io/publication/turk-2010-vh/","publishdate":"2019-05-30T20:07:09.389768Z","relpermalink":"/publication/turk-2010-vh/","section":"publication","summary":"We report on simulations of the formation of the first stars in the","tags":["cosmology: theory; galaxies: formation; H II regions; stars:; formation; Astrophysics - Cosmology and Nongalactic Astrophysics;Authorship"],"title":"High-entropy Polar Regions Around the First Protostars","type":"publication"},{"authors":["Matthew J Turk","Britton D Smith","Jeffrey S Oishi","Stephen Skory","Samuel W Skillman","Tom Abel","Michael L Norman"],"categories":null,"content":"","date":1291161600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"880368f1545659f8c60cd5a61c8a39e5","permalink":"https://matthewturk.github.io/publication/turk-2010-hk/","publishdate":"2019-05-30T20:07:09.388606Z","relpermalink":"/publication/turk-2010-hk/","section":"publication","summary":"The analysis of complex multiphysics astrophysical simulations presents a unique and rapidly growing set of challenges: reproducibility, parallelization, and vast increases in data size and complexity chief among them. In order to meet these challenges, and in order to open up new avenues for collaboration between users of multiple simulation platforms, we present yt (available at http://yt.enzotools.org/) an open source, community-developed astrophysical analysis and visualization toolkit. Analysis and visualization with yt are oriented around physically relevant quantities rather than quantities native to astrophysical simulation codes. While originally designed for handling Enzo's structure adaptive mesh refinement data, yt has been extended to work with several different simulation methods and simulation codes including Orion, RAMSES, and FLASH. We report on its methods for reading, handling, and visualizing data, including projections, multivariate volume rendering, multi-dimensional histograms, halo finding, light cone generation, and topologically connected isocontour identification. Furthermore, we discuss the underlying algorithms yt uses for processing and visualizing data, and its mechanisms for parallelization of analysis tasks.","tags":["Authorship"],"title":"yt: A MULTI-CODE ANALYSIS TOOLKIT FOR ASTROPHYSICAL SIMULATION DATA","type":"publication"},{"authors":["S Skory","M J Turk","M L Norman","A L Coil"],"categories":null,"content":"","date":1262304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"477b00158b1a9bd5ba4c5e21832541b4","permalink":"https://matthewturk.github.io/publication/skory-2010-il/","publishdate":"2019-05-30T20:07:09.377133Z","relpermalink":"/publication/skory-2010-il/","section":"publication","summary":"Modern N-body cosmological simulations contain billions ($10^ 9$) of dark matter particles. These simulations require hundreds to thousands of gigabytes of memory, and employ hundreds to tens of thousands of processing cores on many compute nodes. In order to …","tags":["Authorship"],"title":"Parallel hop: A scalable halo finder for massive cosmological data sets","type":"publication"},{"authors":["Matthew J Turk","Tom Abel","Brian O'Shea"],"categories":null,"content":"","date":1246406400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"ecfd80b5ead375d5b6a7dee166994961","permalink":"https://matthewturk.github.io/publication/turk-2009-fl/","publishdate":"2019-05-30T20:07:09.385796Z","relpermalink":"/publication/turk-2009-fl/","section":"publication","summary":"Previous high-resolution cosmological simulations predicted that the first stars to appear in the early universe were very massive and formed in isolation. Here, we discuss a cosmological simulation in which the central 50 M(o) (where M(o) is the mass of the Sun) clump breaks up into two cores having a mass ratio of two to one, with one fragment collapsing to densities of 10(-8) grams per cubic centimeter. The second fragment, at a distance of approximately 800 astronomical units, is also optically thick to its own cooling radiation from molecular hydrogen lines but is still able to cool via collision-induced emission. The two dense peaks will continue to accrete from the surrounding cold gas reservoir over a period of approximately 10(5) years and will likely form a binary star system.","tags":["Authorship"],"title":"The formation of Population III binaries from cosmological initial conditions","type":"publication"},{"authors":["Britton D Smith","Matthew J Turk","Steinn Sigurdsson","Brian W O'Shea","Michael L Norman"],"categories":null,"content":"","date":1230768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"b1dcadcb3cbb4b53f68bf4f702bd8de8","permalink":"https://matthewturk.github.io/publication/smith-2009-dp/","publishdate":"2019-05-30T20:07:09.384512Z","relpermalink":"/publication/smith-2009-dp/","section":"publication","summary":"Simulations of the formation of Population III (Pop III) stars suggest that they were much more massive than the Pop II and Pop I stars observed today. This is due to the collapse dynamics of metal-free gas, which is regulated by the radiative cooling of molecular hydrogen. We study how the collapse of gas clouds is altered by the addition of metals to the star-forming environment by performing a series of simulations of pre-enriched star formation at various metallicities. To make a clean comparison with metal-free star formation, we use initial conditions identical to a Pop III star formation simulation, with low ionization and no external radiation other than the cosmic microwave background (CMB). For metallicities below the critical metallicity, Z cr, collapse proceeds similar to the metal-free case, and only massive objects form. For metallicities well above Z cr, efficient cooling rapidly lowers the gas temperature to the temperature of the CMB. The gas is unable to radiatively cool below the CMB temperature, and becomes thermally stable. For high metallicities, Z ≳ 10--2.5 Z ☉, this occurs early in the evolution of the gas cloud, when the density is still relatively low. The resulting cloud cores show little or no fragmentation, and would most likely form massive stars. If the metallicity is not vastly above Z cr, the cloud cools efficiently but does not reach the CMB temperature, and fragmentation into multiple objects occurs. We conclude that there were three distinct modes of star formation at high redshift (z ≳ 4): a ``primordial'' mode, producing massive stars (10s to 100s of M ☉) at very low metallicities (Z ≲ 10--3.75 Z ☉); a CMB-regulated mode, producing moderate mass (10s of M ☉) stars at high metallicities (Z ≳ 10--2.5 Z ☉ at redshift z∼ 15-20); and a low-mass (a few M ☉) mode existing between these two metallicities. As the universe ages and the CMB temperature decreases, the range of the low-mass mode extends to higher metallicities, eventually becoming the only mode of star formation.","tags":["Authorship"],"title":"THREE MODES OF METAL-ENRICHED STAR FORMATION IN THE EARLY UNIVERSE","type":"publication"},{"authors":["John H Wise","Matthew J Turk","Tom Abel"],"categories":null,"content":"","date":1228089600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"6e687ba97e6cd867bc232f39e56f3687","permalink":"https://matthewturk.github.io/publication/wise-2008-gl/","publishdate":"2019-05-30T20:07:09.383653Z","relpermalink":"/publication/wise-2008-gl/","section":"publication","summary":"Numerous cosmological hydrodynamic studies have addressed the formation of galaxies. Here we choose to study the first stages of galaxy formation, including nonequilibrium atomic primordial gas cooling, gravity, and hydrodynamics. Using initial conditions appropriate for the concordance cosmological model of structure formation, we perform two adaptive mesh refinement simulations of 108 M☉ galaxies at high redshift. The calculations resolve the Jeans length at all times with more than 16 cells and capture over 14 orders of magnitude in length scales. In both cases, the dense, 105 solar mass, one parsec central regions are found to contract rapidly and have turbulent Mach numbers up to 4. Despite the ever decreasing Jeans length of the isothermal gas, we only find one site of fragmentation during the collapse. However, rotational secular bar instabilities transport angular momentum outward in the central parsec as the gas continues to collapse and lead to multiple nested unstable fragments with decreasing masses down to sub-Jupiter mass scales. Although these numerical experiments neglect star formation and feedback, they clearly highlight the physics of turbulence in gravitationally collapsing gas. The angular momentum segregation seen in our calculations plays an important role in theories that form supermassive black holes from gaseous collapse.","tags":["Authorship"],"title":"Resolving the Formation of Protogalaxies. II. Central Gravitational Collapse","type":"publication"},{"authors":["Matthew J Turk","Tom Abel","Brian W O'Shea"],"categories":null,"content":"","date":1204329600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"41c188ffd7b199c76187c2e05e873a44","permalink":"https://matthewturk.github.io/publication/turk-2008-nk/","publishdate":"2019-05-30T20:07:09.363676Z","relpermalink":"/publication/turk-2008-nk/","section":"publication","summary":"Modeling the formation of the first stars in the universe is a well?posed problem and ideally suited for computational investigation.We have conducted high?resolution numerical studies of the formation of primordial stars. Beginning with primordial initial conditions appropriate for a ?CDM model, we used the Eulerian adaptive mesh refinement code (Enzo) to achieve unprecedented numerical resolution, resolving cosmological scales as well as sub?stellar scales simultaneously. Building on the work of Abel, Bryan and Norman (2002), we followed the evolution of the first collapsing cloud until molecular hydrogen is optically thick to cooling radiation. In addition, the calculations account for the process of collision?induced emission (CIE) and add approximations to the optical depth in both molecular hydrogen roto?vibrational cooling and CIE. Also considered are the effects of chemical heating/cooling from the formation/destruction of molecular hydrogen. We present the results of these simulations, showing the formation of a 10 Jupiter?mass protostellar core bounded by a strongly aspherical accretion shock. Accretion rates are found to be as high as one solar mass per year.","tags":["Authorship"],"title":"Towards Forming a Primordial Protostar in a Cosmological AMR Simulation","type":"publication"},{"authors":["M Turk"],"categories":null,"content":"","date":1199145600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"d444bc751f807abdbc31db0e30dd812f","permalink":"https://matthewturk.github.io/publication/turk-2008-bl/","publishdate":"2019-05-30T20:07:09.374317Z","relpermalink":"/publication/turk-2008-bl/","section":"publication","summary":"The study the origins of cosmic structure requires large-scale computer simulations beginning with well-constrained, observationally-determined, initial conditions. We use Adaptive Mesh Refinement to conduct multi-resolution simulations spanning twelve orders …","tags":["Authorship"],"title":"Analysis and visualization of multi-scale astrophysical simulations using python and numpy","type":"publication"}]