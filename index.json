[{"authors":["admin"],"categories":null,"content":"Matthew Turk is an assistant professor in the School of Information Sciences and also holds an appointment with the Department of Astronomy in the College of Liberal Arts and Sciences. His research is focused on how individuals interact with data and how that data is processed and understood.\nAt the University of Illinois, he leads the Data Exploration Lab and teaches in Data Visualization, Data Storytelling, and Computational Astrophysics.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1559268143,"objectID":"598b63dd58b43bce02403646f240cd3c","permalink":"https://matthewturk.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"author","summary":"Matthew Turk is an assistant professor in the School of Information Sciences and also holds an appointment with the Department of Astronomy in the College of Liberal Arts and Sciences. His research is focused on how individuals interact with data and how that data is processed and understood.\nAt the University of Illinois, he leads the Data Exploration Lab and teaches in Data Visualization, Data Storytelling, and Computational Astrophysics.","tags":null,"title":"Matthew Turk","type":"author"},{"authors":[],"categories":[],"content":" In the still-in-development version of yt (4.0), the way that particles are handled has been redesigned from the ground up.\nThe current version of yt (3.x) utilizes an octree-based approach for meshing the particles, although not for indexing them \u0026ndash; which presents some problems when doing subsets of particles, as well as when doing visualizations that rely on an implicit meshing. The main result is that, in general, particle visualizations in yt 3.x aren\u0026rsquo;t that great, and are underresolved.\nIn yt 4.0, the particle system has been reimplemented to use EWAH bitmap indices (for more info, see Daniel Lemire\u0026rsquo;s EWAHBoolArray repository) to track which \u0026ldquo;regions\u0026rdquo; of files correspond to particular spatial regions, as designated by indices in a space-filling curve. Things are now orders of magnitude faster to load, to subset, and to visualize \u0026ndash; and the memory overhead is so much lower!\nThis work was led by Nathan Goldbaum and Meagan Lang, with crucial contributions from the rest of the yt community, including feedback and bugfixes from Bili Dong and Cameron Hummels.\nRecently, I\u0026rsquo;ve been exploring using a different array backend in yt, right now focusing on dask. While yt does lots of MPI-parallel operations, much of what we do with these has to be hand-programmed \u0026ndash; so when you implement a new DerivedQuantity (i.e., stuff like calling min on a data object) you have to jump through a few hoops related to intermediate values and the like. Plus, dask seems to be everywhere, and so if we exported to dask arrays or somehow interoperated better with it, we\u0026rsquo;d be able to interoperate with lots of the rest of the ecosystem more easily.\nUnfortunately, there\u0026rsquo;s a bit of an impedance mismatch which \u0026hellip; has made this more difficult than I\u0026rsquo;d like.\nReading Data Before getting too much further, though, I\u0026rsquo;m going to go through a bit about how yt thinks about \u0026ldquo;chunking\u0026rdquo; data.\nThe fundamental thing that yt does is index data. (Well, that, and take a while to compile all the Cython code.) Processing of the data is all layered on top of that \u0026ndash; including some pretty cool semantics-of-data and units, visualization, etc. The main thing is that if you do a subset, it knows where to go to grab that subset of data, and if you want to do something that touches everything, it\u0026rsquo;ll do its best to reduce the number of times data is loaded off disk in service of that.\nWe do this with a \u0026ldquo;chunking\u0026rdquo; system, which is implemented differently if your data is discrete (i.e., particles), mesh-based, and so on.\nSo to show what the problem is, I\u0026rsquo;m going to load up a dataset from the FIRE project.\nimport yt ds = yt.load(\u0026quot;data/FIRE_M12i_ref11/snapshot_600.hdf5\u0026quot;)  yt : [INFO ] 2019-06-02 16:02:22,303 Calculating time from 1.000e+00 to be 4.355e+17 seconds yt : [INFO ] 2019-06-02 16:02:22,304 Assuming length units are in kpc/h (comoving) yt : [INFO ] 2019-06-02 16:02:22,337 Parameters: current_time = 4.3545571088051386e+17 s yt : [INFO ] 2019-06-02 16:02:22,338 Parameters: domain_dimensions = [1 1 1] yt : [INFO ] 2019-06-02 16:02:22,339 Parameters: domain_left_edge = [0. 0. 0.] yt : [INFO ] 2019-06-02 16:02:22,341 Parameters: domain_right_edge = [60000. 60000. 60000.] yt : [INFO ] 2019-06-02 16:02:22,342 Parameters: cosmological_simulation = 1 yt : [INFO ] 2019-06-02 16:02:22,343 Parameters: current_redshift = 0.0 yt : [INFO ] 2019-06-02 16:02:22,344 Parameters: omega_lambda = 0.728 yt : [INFO ] 2019-06-02 16:02:22,344 Parameters: omega_matter = 0.272 yt : [INFO ] 2019-06-02 16:02:22,345 Parameters: omega_radiation = 0.0 yt : [INFO ] 2019-06-02 16:02:22,347 Parameters: hubble_constant = 0.702  At this point yt has done a tiny little bit of reading of the data \u0026ndash; just enough to figure out some of the metadata. It hasn\u0026rsquo;t indexed anything yet or read any of the actual data fields off of disk.\nNow let\u0026rsquo;s make a plot of the gas density, integrated over the z axis of the simulation. Keep in mind that in doing this, it will have to read all the gas particles and smooth them onto a buffer. The first time this gets run, an index is generated and then stored to disk. More on that in a moment.\nI\u0026rsquo;m going to use ds.r[:] here for \u0026ldquo;dataset region, but the whole thing\u0026rdquo; and then I call integrate on it and specify the field to integrate. Then, I plot it.\np=ds.r[:].integrate(\u0026quot;density\u0026quot;, axis=\u0026quot;z\u0026quot;).plot((\u0026quot;gas\u0026quot;, \u0026quot;density\u0026quot;))  yt : [INFO ] 2019-06-02 16:02:22,484 Allocating for 4.787e+06 particles Loading particle index: 100%|██████████| 10/10 [00:00\u0026lt;00:00, 817.25it/s] yt : [INFO ] 2019-06-02 16:02:23,623 xlim = 0.000000 60000.000000 yt : [INFO ] 2019-06-02 16:02:23,623 ylim = 0.000000 60000.000000 yt : [INFO ] 2019-06-02 16:02:23,633 Making a fixed resolution buffer of (('gas', 'density')) 800 by 800  (All that empty space is because there are only gas particles in the middle of the dataset!)\nThe first time any data needs to be read from a particle dataset, yt will construct an in-memory index of the data on disk; by default, it will store this in a sidecar file, so the next time that the dataset is read it does not need to be generated again.\nThe way the bitmap indices work is really fun, but that deserves its own blog post. It suffices to say that the indexing helps to figure out both which files to read, and which subsets of those files to read, since we don\u0026rsquo;t assume that the particles are sorted in any way. (Mostly because each code tends to sort the particles in its own way!)\nNow, for projecting over the whole domain, it\u0026rsquo;s not that big a deal to read everything, since we have to anyway, but if we did a subset it could dramatically reduce the IO necessary, and it also keeps much less data resident in memory than the old implementation.\nContinuing on, let\u0026rsquo;s say that we now want to center at a different location. We\u0026rsquo;d figure out the most dense point, and then set our center.\nc = ds.r[:].argmax((\u0026quot;gas\u0026quot;, \u0026quot;density\u0026quot;))  (One thing this next set of code highlights is that, in general, how we handle centers in yt is a bit clumsy at times. Writing this blog post led me to filing an issue which may or may not get any traction or support.)\np.set_origin(\u0026quot;center-window\u0026quot;) p.set_center((c[0], c[1])) p.zoom(25) p.set_zlim((\u0026quot;gas\u0026quot;,\u0026quot;density\u0026quot;), 1e-6, 1e-3)  yt : [INFO ] 2019-06-02 16:02:25,607 xlim = -713.911179 59286.088821 yt : [INFO ] 2019-06-02 16:02:25,611 ylim = 1049.283652 61049.283652 yt : [INFO ] 2019-06-02 16:02:25,619 Making a fixed resolution buffer of (('gas', 'density')) 800 by 800  So, we can visualize now, and it\u0026rsquo;s faster than it was before, and we also get much better results. Great. So why am I belaboring this point?\nIt\u0026rsquo;s because in the background, yt is queryin a data object to see which items to read off disk, then it is reading those items off disk. In this particular instance, it is doing what we call \u0026ldquo;io\u0026rdquo; chunking \u0026ndash; this means to use whatever type of hinting is best to get the most efficient ordering it knows how. Among other things, yt will try to minimize the number of times it opens a file, it seeks in a file, and it tries to keep the memory allocation count as low as possible.\n(I\u0026rsquo;ll write more on this last point later \u0026ndash; much of what yt does to index in yt-3.x and yt-4.0 is designed to keep the number of allocated arrays in the IO routines as low as possible, and to avoid any expensive concatenation or subselection operations. It turns out, this is \u0026hellip; not as big a deal as thought when this was made a design principle. And in general, it leads to a lot more floating point operations than we would like, and sometimes more stuff in memory, too.)\nAnd, so, uh, \u0026ldquo;chunking\u0026rdquo; is\u0026hellip;? We can figure out how yt chunks this data by, well, asking it to do it manually! Every data object presents a chunks interface which is a generator that modifies its internal state and then yields itself. For instance:\ndd = ds.all_data() for chunk in dd.chunks([], \u0026quot;io\u0026quot;): print(chunk[\u0026quot;particle_ones\u0026quot;].size)  1048576 885527 753678 524288 317696 262144 262144 262144 262144 208609  I mentioned that this generator yields itself; this is true. But the internal state is modified to store where we are in the iteration, along with things like the parameters for derived fields and the like. The source for this looks like this:\nfrom yt.data_objects.data_containers import YTSelectionContainer YTSelectionContainer.chunks??  Signature: YTSelectionContainer.chunks(self, fields, chunking_style, **kwargs) Docstring: \u0026lt;no docstring\u0026gt; Source: def chunks(self, fields, chunking_style, **kwargs): # This is an iterator that will yield the necessary chunks. self.get_data() # Ensure we have built ourselves if fields is None: fields = [] # chunk_ind can be supplied in the keyword arguments. If it's a # scalar, that'll be the only chunk that gets returned; if it's a list, # those are the ones that will be. chunk_ind = kwargs.pop(\u0026quot;chunk_ind\u0026quot;, None) if chunk_ind is not None: chunk_ind = ensure_list(chunk_ind) for ci, chunk in enumerate(self.index._chunk(self, chunking_style, **kwargs)): if chunk_ind is not None and ci not in chunk_ind: continue with self._chunked_read(chunk): self.get_data(fields) # NOTE: we yield before releasing the context yield self File: ~/yt/yt/yt/data_objects/data_containers.py Type: function  Note that this relies on the index object providing the _chunk routine, which interprets the type of chunking. Also, _chunked_read is a context manager which looks like this:\nYTSelectionContainer._chunked_read??  Signature: YTSelectionContainer._chunked_read(self, chunk) Docstring: \u0026lt;no docstring\u0026gt; Source: @contextmanager def _chunked_read(self, chunk): # There are several items that need to be swapped out # field_data, size, shape obj_field_data = [] if hasattr(chunk, 'objs'): for obj in chunk.objs: obj_field_data.append(obj.field_data) obj.field_data = YTFieldData() old_field_data, self.field_data = self.field_data, YTFieldData() old_chunk, self._current_chunk = self._current_chunk, chunk old_locked, self._locked = self._locked, False yield self.field_data = old_field_data self._current_chunk = old_chunk self._locked = old_locked if hasattr(chunk, 'objs'): for obj in chunk.objs: obj.field_data = obj_field_data.pop(0) File: ~/yt/yt/yt/data_objects/data_containers.py Type: function  This is a bit clunky, but it stores the old state (because, believe it or not, sometimes we have multiple levels of chunking simultaneously, especially for things like spatial derivatives) and then it makes a fresh state, and then it resets it after the context manager concludes.\nSo the end result here is that we have a mechanism that divides the dataset up into the chunks it needs (YTDataChunk objects), and then iterates over them. What does this look like for our particle dataset? Well, we can find out, evidently, by looking at the _current_chunk attribute on the object yielded by chunks.\nI\u0026rsquo;ve changed what we print out here just a little bit, because I want to keep the output a bit more human readable, but this is what it looks like:\ndd = ds.all_data() for chunk in dd.chunks([], \u0026quot;io\u0026quot;): print(\u0026quot;\\nExamining chunk...\u0026quot;) for obj in chunk._current_chunk.objs: print(\u0026quot; Examining obj...\u0026quot;,) for data_file in obj.data_files: print(\u0026quot; {}: {}-{}\u0026quot;.format(data_file.filename, data_file.start, data_file.end))  Examining chunk... Examining obj... /home/matthewturk/data/FIRE_M12i_ref11/snapshot_600.hdf5: 0-262144 Examining chunk... Examining obj... /home/matthewturk/data/FIRE_M12i_ref11/snapshot_600.hdf5: 262144-524288 Examining chunk... Examining obj... /home/matthewturk/data/FIRE_M12i_ref11/snapshot_600.hdf5: 524288-786432 Examining chunk... Examining obj... /home/matthewturk/data/FIRE_M12i_ref11/snapshot_600.hdf5: 786432-1048576 Examining chunk... Examining obj... /home/matthewturk/data/FIRE_M12i_ref11/snapshot_600.hdf5: 1048576-1310720 Examining chunk... Examining obj... /home/matthewturk/data/FIRE_M12i_ref11/snapshot_600.hdf5: 1310720-1572864 Examining chunk... Examining obj... /home/matthewturk/data/FIRE_M12i_ref11/snapshot_600.hdf5: 1572864-1835008 Examining chunk... Examining obj... /home/matthewturk/data/FIRE_M12i_ref11/snapshot_600.hdf5: 1835008-2097152 Examining chunk... Examining obj... /home/matthewturk/data/FIRE_M12i_ref11/snapshot_600.hdf5: 2097152-2359296 Examining chunk... Examining obj... /home/matthewturk/data/FIRE_M12i_ref11/snapshot_600.hdf5: 2359296-2567905  A few notes here. Each chunk is able to have multiple \u0026ldquo;objects\u0026rdquo; associated with it (which in grid frontends usually means multiple grid objects) but here, we have only one entry in the obj list associated with each. Each obj then only has one item in data_files, which is not really a data file, but instead a subset of a data file specified by its start and end indices.\nIf you\u0026rsquo;re thinking this is a bit clumsy, I would agree with you.\nDask Stuff The issue that I wrote about at the start of this blog post shows up when we start looking at how these chunks are generated. In principle, this does not map that badly to how dask expect chunks to be emitted.\n(At this point I need to admit that while I\u0026rsquo;ve worked with dask, it\u0026rsquo;s entirely possible that I am going to misrepresent its capabilities. Any errors are my own, and if I find out I am mistaken about any of this, I will happily update this blog post!)\nIt\u0026rsquo;s possible to create a dask array through the dask.array.Array constructor; this is described in the array design docs. Since yt uses unyt for attaching units we will need to do some additional work, but let\u0026rsquo;s imagine that we are simply happy dealing with unit-less (and, I suppose, unyt-less) arrays for now.\nTo generate these arrays most efficiently, we need to be able to specify their size, how to obtain them, and maybe a couple other things. But for our purposes, those are the two most important things.\nUnfortunately, as you might be able to tell, this is not information that is super easily exposed without iterating over the dataset. Sure, if we iterated and read everything, of course we can show the appropriate info. And, I posted a little bit about how one might do this on issue 1891, but there\u0026rsquo;s a key thing going on in that code \u0026ndash; yt has already read all the data from disk.\nSo, this isn\u0026rsquo;t ideal.\nChunks are not persistent This all comes about because chunks are not persistent, and more specifically, chunks are always create on-demand. Each different data object will have its own set of chunks, and these will map differently. So, for instance, we might end up selecting all the same sets of objects, but they will have different sizes (and even each different field might be a different size).\nsp1 = ds.sphere(c, (1, \u0026quot;Mpc\u0026quot;)) sp2 = ds.r[ (20.0, \u0026quot;Mpc\u0026quot;) : (40.0, \u0026quot;Mpc\u0026quot;), (25.0, \u0026quot;Mpc\u0026quot;) : (45.0, \u0026quot;Mpc\u0026quot;), (55.0, \u0026quot;Mpc\u0026quot;) : (65.0, \u0026quot;Mpc\u0026quot;) ] print(\u0026quot;sp1 len == {}\\nsp2 len == {}\u0026quot;.format( len(list(sp1.chunks([], \u0026quot;io\u0026quot;))), len(list(sp2.chunks([], \u0026quot;io\u0026quot;))) )) print(\u0026quot;sp1 =\u0026gt; \u0026quot;, \u0026quot; \u0026quot;.join(str(chunk[\u0026quot;particle_ones\u0026quot;].size) for chunk in sp1.chunks([], \u0026quot;io\u0026quot;))) print(\u0026quot;sp2 =\u0026gt; \u0026quot;, \u0026quot; \u0026quot;.join(str(chunk[\u0026quot;particle_ones\u0026quot;].size) for chunk in sp2.chunks([], \u0026quot;io\u0026quot;)))  sp1 len == 10 sp2 len == 10 sp1 =\u0026gt; 388571 306586 341808 205880 50260 2 1 2 3 0 sp2 =\u0026gt; 12 3673 480 29 146 200 77 419 3697 400  The trickiest part of this is that in these cases, we don\u0026rsquo;t know how big each one is going to be! For other types of indexing, it\u0026rsquo;s slightly different \u0026ndash; the indexing system for grids and octrees and meshes can figure out in advance (without reading data from disk) the precise number of values that will be read. But for particles we don\u0026rsquo;t necessarily know.\nUnfortunately, even if we did, the way that the YTDataChunk objects are the result of creating, then yield-ing, rather than returning a list of objects with known sizes makes it harder to expose this to dask. In particular, because we can\u0026rsquo;t (inexpensively) fast-forward the generator or rewind it or even access it elementwise makes it tricky to interface. One can expose unknown chunk sizes to dask, but it seems like we could do better.\nSo what can be done? Well, let me first note that a lot of this is a result of trying to be clever! Back when the chunking system was being implemented, it seemed like simple generator expressions were the right way to do it. And, a bunch of layers have been added on top of those generator expressions that make it harder to simply strip that component out.\nBut recently, Britton Smith and I have been digging into some of the particle frontends, and we think we might have a solution that would both simplify a lot of this logic and make it a lot easier to expose the arrays to different array backends \u0026ndash; specifically dask.\nFor more on that, wait for part two!\n","date":1559340372,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559509871,"objectID":"e2409a44c1f2fb08fb0b73f5ab5788e4","permalink":"https://matthewturk.github.io/post/refactoring-yt-frontends-part1/","publishdate":"2019-05-31T17:06:12-05:00","relpermalink":"/post/refactoring-yt-frontends-part1/","section":"post","summary":"The first post in a deep dive into yt frontends, chunking, and why and how they might be refactored.","tags":[],"title":"Refactoring yt Frontends - Part 1","type":"post"},{"authors":null,"categories":null,"content":"","date":1557177039,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559249134,"objectID":"2fd5b0f658881a3f851b03da682bc48d","permalink":"https://matthewturk.github.io/project/yt/","publishdate":"2019-05-06T16:10:39-05:00","relpermalink":"/project/yt/","section":"project","summary":"yt is an open-source python package for analyzing and visualizing volumetric data.","tags":[],"title":"yt","type":"project"},{"authors":[],"categories":null,"content":"","date":1556736856,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557177314,"objectID":"5f15fff70af789e7da9d4a78640dfa59","permalink":"https://matthewturk.github.io/talk/2019-05-01-crops-dependencies/","publishdate":"2019-05-01T13:54:16-05:00","relpermalink":"/talk/2019-05-01-crops-dependencies/","section":"talk","summary":"This is a brief overview of how we can think about dependencies in the Crops in Silico project, and how we can use that to organize our work and collaboration.","tags":[],"title":"Crops-in-Silico Collaboration and Dependencies","type":"talk"},{"authors":["Adam Brinckman","Kyle Chard","Niall Gaffney","Mihael Hategan","Matthew B Jones","Kacper Kowalik","Sivakumar Kulasekaran","Bertram Ludäscher","Bryce D Mecum","Jarek Nabrzyski","Victoria Stodden","Ian J Taylor","Matthew J Turk","Kandace Turner"],"categories":null,"content":"","date":1556668800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"4d610cc35353e551e4dd5786294bba6d","permalink":"https://matthewturk.github.io/publication/brinckman-2019-oa/","publishdate":"2019-05-30T20:07:09.390766Z","relpermalink":"/publication/brinckman-2019-oa/","section":"publication","summary":"The act of sharing scientific knowledge is rapidly evolving away from traditional articles and presentations to the delivery of executable objects that integrate the data and computational details (e.g., scripts and workflows) upon which the findings rely. This envisioned coupling of data and process is essential to advancing science but faces technical and institutional barriers. The Whole Tale project aims to address these barriers by connecting computational, data-intensive research efforts with the larger research process---transforming the knowledge discovery and dissemination process into one where data products are united with research articles to create ``living publications'' or tales. The Whole Tale focuses on the full spectrum of science, empowering users in the long tail of science, and power users with demands for access to big data and compute resources. We report here on the design, architecture, and implementation of the Whole Tale environment.","tags":["Living publications; Reproducibility; Provenance; Data sharing; Code sharing;Authorship"],"title":"Computing environments for reproducibility: Capturing the ``Whole Tale''","type":"publication"},{"authors":[],"categories":null,"content":"","date":1556082000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557177314,"objectID":"8e090a510726760f69cb7803cc7fcc0c","permalink":"https://matthewturk.github.io/talk/2019-04-24-ddd-update/","publishdate":"2019-05-01T13:52:48-05:00","relpermalink":"/talk/2019-04-24-ddd-update/","section":"talk","summary":"My 'update' talk at the Moore Data Driven Discovery Investigator Symposium in April, 2019.","tags":[],"title":"DDD Update","type":"talk"},{"authors":[],"categories":null,"content":"","date":1554058436,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557177314,"objectID":"3e44c17136c4c894688a3bcd35473a56","permalink":"https://matthewturk.github.io/talk/2019-03-31-troubleshooting-data-storytelling/","publishdate":"2019-05-01T13:53:56-05:00","relpermalink":"/talk/2019-03-31-troubleshooting-data-storytelling/","section":"talk","summary":"","tags":[],"title":"Troubleshooting Data Storytelling","type":"talk"},{"authors":[],"categories":null,"content":"","date":1553893200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557177314,"objectID":"b4dbdf946084cccc2be51a72048b1513","permalink":"https://matthewturk.github.io/talk/2019-03-29-cirss-open-source-teen-years/","publishdate":"2019-04-30T17:34:14-05:00","relpermalink":"/talk/2019-03-29-cirss-open-source-teen-years/","section":"talk","summary":"In this talk, I will reflect on experiences I have had navigating the landscape of open source scholarly software as projects age, and the way that shapes interaction in an ecosystem.  I will also report some recent developments with the open source project yt, and how they both interact with the shifting needs and desires of community members and how they address the needs and desires of potential future community members.  Many exciting buzzwords such as 'Rust' and 'WebAssembly' will be used.","tags":[],"title":"CIRSS Seminar: Open Source in the Teen Years","type":"talk"},{"authors":null,"categories":null,"content":"","date":1549052179,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559249134,"objectID":"4600419641aba970b03879da5587bed8","permalink":"https://matthewturk.github.io/project/crops-in-silico/","publishdate":"2019-02-01T15:16:19-05:00","relpermalink":"/project/crops-in-silico/","section":"project","summary":"Crops in Silico is an integrative and multi-scale modeling platform to combine modeling efforts toward the generation of virtual crops, open and accessible to the global community.","tags":[],"title":"Crops in Silico","type":"project"},{"authors":null,"categories":null,"content":"","date":1548879389,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559249134,"objectID":"670e9b7050687e20c063667daf003870","permalink":"https://matthewturk.github.io/project/whole-tale/","publishdate":"2019-01-30T15:16:29-05:00","relpermalink":"/project/whole-tale/","section":"project","summary":"Whole Tale is an initiative to build a scalable, open source, web-based, multi-user platform for reproducible research.","tags":[],"title":"Whole Tale","type":"project"},{"authors":null,"categories":null,"content":"This course covered topics that could broadly be described as \u0026ldquo;advanced,\u0026rdquo; including new platforms and tools for visualizing data, and how to present data in different, more thoughtful ways. It was structured differently than the other data viz courses, and designed for more interaction with a smaller group of students.\n","date":1546383114,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559268143,"objectID":"81e4db4ee0b05df2af767d7e3ba9a362","permalink":"https://matthewturk.github.io/courses/is590adv-spr2019/","publishdate":"2019-01-01T17:51:54-05:00","relpermalink":"/courses/is590adv-spr2019/","section":"courses","summary":"Seminar on advanced or in-depth topics in data visualization","tags":[],"title":"IS590ADV - Spring 2019","type":"courses"},{"authors":null,"categories":null,"content":"This course, offered in Fall of 2018, included more javascript than previous iterations and also utilized bqplot to a greater extent.\n","date":1533163907,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559268143,"objectID":"edece407444789734693d605192694b5","permalink":"https://matthewturk.github.io/courses/is590dv-fall2018/","publishdate":"2018-08-01T17:51:47-05:00","relpermalink":"/courses/is590dv-fall2018/","section":"courses","summary":"Data Viz from Fall 2018","tags":[],"title":"IS590DV - Fall 2018","type":"courses"},{"authors":["Nathan J Goldbaum","John A ZuHone","Matthew J Turk","Kacper Kowalik","Anna L Rosen"],"categories":null,"content":"","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"abc200f366401cf32068cf880c72c3bb","permalink":"https://matthewturk.github.io/publication/goldbaum-2018-ws/","publishdate":"2019-05-30T20:07:09.372097Z","relpermalink":"/publication/goldbaum-2018-ws/","section":"publication","summary":"Software that processes real-world data or that models a physical system must have some way of managing units. While simple approaches like the understood convention that all data are in a unit system (such as the MKS SI unit system) do work in practice, they are fraught with possible sources of error both by developers and users of the software. In this paper we present unyt, a Python library based on NumPy and SymPy for handling data that has units. It is designed both to aid quick interactive calculations and to be tightly integrated into a larger Python application or library. We compare unyt with two other Python libraries for handling units, Pint and astropy.units, and find that unyt is faster, has higher test coverage, and has fewer lines of code.","tags":["Authorship"],"title":"unyt: Handle, manipulate, and convert data with units in Python","type":"publication"},{"authors":null,"categories":null,"content":"This was an experimental course designed to convey the basics of \u0026ldquo;conversational computation\u0026rdquo; to astronomy undergraduates.\n","date":1514847094,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559268143,"objectID":"a66798ccee3cefb7f8c172b6f85651f8","permalink":"https://matthewturk.github.io/courses/astr496-spr2018/","publishdate":"2018-01-01T17:51:34-05:00","relpermalink":"/courses/astr496-spr2018/","section":"courses","summary":"Introduction to Computational Astrophysics","tags":[],"title":"ASTR496 - Spring 2018","type":"courses"},{"authors":null,"categories":null,"content":"Starting in Spring 2018, I transitioned courses to using Github Pages and RevealJS. The repository includes built slide decks and rendered Jupyter notebooks.\n","date":1514847088,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559268143,"objectID":"106a69791a1c8e84dba3b3f48bdfd470","permalink":"https://matthewturk.github.io/courses/is590dv-spr2018/","publishdate":"2018-01-01T17:51:28-05:00","relpermalink":"/courses/is590dv-spr2018/","section":"courses","summary":"Data Viz from Spring 2018","tags":[],"title":"IS590DV - Spring 2018","type":"courses"},{"authors":["Hsi-Yu Schive","John A ZuHone","Nathan J Goldbaum","Matthew J Turk","Massimo Gaspari","Chin-Yu Cheng"],"categories":null,"content":"","date":1512086400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"3b5789604fd9a3ad84d80b2d4e03c5fe","permalink":"https://matthewturk.github.io/publication/schive-2017-ep/","publishdate":"2019-05-30T20:07:09.39481Z","relpermalink":"/publication/schive-2017-ep/","section":"publication","summary":"We present GAMER-2, a GPU-accelerated adaptive mesh refinement (AMR) code for astrophysics. It provides a rich set of features, including adaptive time-stepping, several hydrodynamic schemes, magnetohydrodynamics, self-gravity, particles, star formation, chemistry and radiative processes with GRACKLE, data analysis with yt, and memory pool for efficient object allocation. GAMER-2 is fully bitwise reproducible. For the performance optimization, it adopts hybrid OpenMP/MPI/GPU parallelization and utilizes overlapping CPU computation, GPU computation, and CPU-GPU communication. Load balancing is achieved using a Hilbert space-filling curve on a level-by-level basis without the need to duplicate the entire AMR hierarchy on each MPI process. To provide convincing demonstrations of the accuracy and performance of GAMER-2, we directly compare with Enzo on isolated disk galaxy simulations and with FLASH on galaxy cluster merger simulations. We show that the physical results obtained by different codes are in very good agreement, and GAMER-2 outperforms Enzo and FLASH by nearly one and two orders of magnitude, respectively, on the Blue Waters supercomputers using $1-256$ nodes. More importantly, GAMER-2 exhibits similar or even better parallel scalability compared to the other two codes. We also demonstrate good weak and strong scaling using up to 4096 GPUs and 65,536 CPU cores, and achieve a uniform resolution as high as $10,240^3$ cells. Furthermore, GAMER-2 can be adopted as an AMR+GPUs framework and has been extensively used for the wave dark matter ($ψ$DM) simulations. GAMER-2 is open source (available at https://github.com/gamer-project/gamer) and new contributions are welcome.","tags":["Authorship;DXL Member Papers"],"title":"GAMER-2: a GPU-accelerated adaptive mesh refinement code -- accuracy, performance, and scalability","type":"publication"},{"authors":null,"categories":null,"content":"This was the second semester that I taught Data Viz, and I was still largely using Google Slides. The link to the repository includes the lecture PDFs and notebooks used.\n","date":1501627863,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559268143,"objectID":"2882197390cd2fb552718a0c7b83241b","permalink":"https://matthewturk.github.io/courses/lis590dv-fall2017/","publishdate":"2017-08-01T17:51:03-05:00","relpermalink":"/courses/lis590dv-fall2017/","section":"courses","summary":"Data Viz from Fall 2017","tags":[],"title":"LIS590DV - Fall 2017","type":"courses"},{"authors":["Amy Marshall-Colon","Stephen P Long","Douglas K Allen","Gabrielle Allen","Daniel A Beard","Bedrich Benes","Susanne von Caemmerer","A J Christensen","Donna J Cox","John C Hart","Peter M Hirst","Kavya Kannan","Daniel S Katz","Jonathan P Lynch","Andrew J Millar","Balaji Panneerselvam","Nathan D Price","Przemyslaw Prusinkiewicz","David Raila","Rachel G Shekar","Stuti Shrivastava","Diwakar Shukla","Venkatraman Srinivasan","Mark Stitt","Matthew J Turk","Eberhard O Voit","Yu Wang","Xinyou Yin","Xin-Guang Zhu"],"categories":null,"content":"","date":1493596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"bb0989fb716998e413388479a9bd5b8e","permalink":"https://matthewturk.github.io/publication/marshall-colon-2017-fb/","publishdate":"2019-05-30T20:07:09.377714Z","relpermalink":"/publication/marshall-colon-2017-fb/","section":"publication","summary":"Multi-scale models can facilitate whole plant simulations by linking gene networks, protein synthesis, metabolic pathways, physiology, and growth. Whole plant models can be further integrated with ecosystem, weather, and climate models to predict how various interactions respond to environmental perturbations. These models have the potential to fill in missing mechanistic details and generate new hypotheses to prioritize directed engineering efforts. Outcomes will potentially accelerate improvement of crop yield, sustainability, and increase future food security. It is time for a paradigm shift in plant modeling, from largely isolated efforts to a connected community that takes advantage of advances in high performance computing and mechanistic understanding of plant processes. Tools for guiding future crop breeding and engineering, understanding the implications of discoveries at the molecular level for whole plant behavior, and improved prediction of plant and ecosystem responses to the environment are urgently needed. The purpose of this perspective is to introduce Crops in silico (cropsinsilico.org), an integrative and multi-scale modeling platform, as one solution that combines isolated modeling efforts toward the generation of virtual crops, which is open and accessible to the entire plant biology community. The major challenges involved both in the development and deployment of a shared, multi-scale modeling platform, which are summarized in this prospectus, were recently identified during the first Crops in silico Symposium and Workshop.","tags":["computational framework; crop yield; integration; model; multiscale;Authorship"],"title":"Crops In Silico: Generating Virtual Crops Using an Integrative and Multi-scale Modeling Platform","type":"publication"},{"authors":["Britton D Smith","Greg L Bryan","Simon C O Glover","Nathan J Goldbaum","Matthew J Turk","John Regan","John H Wise","Hsi-Yu Schive","Tom Abel","Andrew Emerick","Brian W O'Shea","Peter Anninos","Cameron B Hummels","Sadegh Khochfar"],"categories":null,"content":"","date":1491004800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"01c869b2c02e627571168f1e7ea367d2","permalink":"https://matthewturk.github.io/publication/smith-2017-za/","publishdate":"2019-05-30T20:07:09.393935Z","relpermalink":"/publication/smith-2017-za/","section":"publication","summary":"We present the grackle chemistry and cooling library for astrophysical simulations and models. grackle provides a treatment of non-equilibrium primordial chemistry and cooling for H, D and He species, including H2 formation on dust grains; tabulated primordial and metal cooling; multiple ultraviolet background models; and support for radiation transfer and arbitrary heat sources. The library has an easily implementable interface for simulation codes written in c, c++ and fortran as well as a python interface with added convenience functions for semi-analytical models. As an open-source project, grackle provides a community resource for accessing and disseminating astrochemical data and numerical methods. We present the full details of the core functionality, the simulation and python interfaces, testing infrastructure, performance and range of applicability. grackle is a fully open-source project and new contributions are welcome.","tags":["Authorship;DXL Member Papers"],"title":"grackle: a chemistry and cooling library for astrophysics","type":"publication"},{"authors":null,"categories":null,"content":"This was the first time I taught Data Viz, and the course repository includes the original Google Slides PDFs, links to the presentations, and the notebooks used in the course.\n","date":1483246801,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559268143,"objectID":"a8ae1d547cdc224350917d8d2bd536a0","permalink":"https://matthewturk.github.io/courses/lis590dv-spr2017/","publishdate":"2017-01-01T00:00:01-05:00","relpermalink":"/courses/lis590dv-spr2017/","section":"courses","summary":"Data Viz from Spring 2017","tags":[],"title":"LIS590DV - Spring 2017","type":"courses"},{"authors":["Harshil M Kamdar","Matthew J Turk","Robert J Brunner"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"d2de67e520898676ea8c2ae39f8eaf22","permalink":"https://matthewturk.github.io/publication/kamdar-2016-ae/","publishdate":"2019-05-30T20:07:09.37875Z","relpermalink":"/publication/kamdar-2016-ae/","section":"publication","summary":"We present a new exploratory framework to model galaxy formation and evolution in a hierarchical Universe by using machine learning (ML). Our motivations are two-fold: (1) presenting a new, promising technique to study galaxy formation, and (2) quantitatively analysing the extent of the influence of dark matter halo properties on galaxies in the backdrop of semi-analytical models (SAMs). We use the influential Millennium Simulation and the corresponding Munich SAM to train and test various sophisticated ML algorithms (k-Nearest Neighbors, decision trees, random forests, and extremely randomized trees). By using only essential dark matter halo physical properties for haloes of M  1012 M⊙ and a partial merger tree, our model predicts the hot gas mass, cold gas mass, bulge mass, total stellar mass, black hole mass and cooling radius at z = 0 for each central galaxy in a dark matter halo for the Millennium run. Our results provide a unique and powerful phenomenological framework to explore the galaxy--halo connection that is built upon SAMs and demonstrably place ML as a promising and a computationally efficient tool to study small-scale structure formation.","tags":["Authorship"],"title":"Machine learning and cosmological simulations -- I. Semi-analytical models","type":"publication"},{"authors":["Harshil M Kamdar","Matthew J Turk","Robert J Brunner"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"0e563eede49bd354c26f1fd53f232568","permalink":"https://matthewturk.github.io/publication/kamdar-2016-mu/","publishdate":"2019-05-30T20:07:09.376485Z","relpermalink":"/publication/kamdar-2016-mu/","section":"publication","summary":"We extend a machine learning (ML) framework presented previously to model galaxy formation and evolution in a hierarchical universe using N-body+ hydrodynamical simulations. In this work, we show that ML is a promising technique to study galaxy formation …","tags":["Authorship"],"title":"Machine learning and cosmological simulations--II. Hydrodynamical simulations","type":"publication"},{"authors":["Desika Narayanan","Matthew Turk","Robert Feldmann","Thomas Robitaille","Philip Hopkins","Robert Thompson","Christopher Hayward","David Ball","Claude-André Faucher-Giguère","Dušan Kereš"],"categories":null,"content":"","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"f598bdf0862f455746a898a59f677c1a","permalink":"https://matthewturk.github.io/publication/narayanan-2015-gq/","publishdate":"2019-05-30T20:07:09.392775Z","relpermalink":"/publication/narayanan-2015-gq/","section":"publication","summary":"Submillimetre-bright galaxies at high redshift are the most luminous, heavily star-forming galaxies in the Universe and are characterized by prodigious emission in the far-infrared, with a flux of at least five millijanskys at a wavelength of 850 micrometres. They reside in haloes with masses about 10(13) times that of the Sun, have low gas fractions compared to main-sequence disks at a comparable redshift, trace complex environments and are not easily observable at optical wavelengths. Their physical origin remains unclear. Simulations have been able to form galaxies with the requisite luminosities, but have otherwise been unable to simultaneously match the stellar masses, star formation rates, gas fractions and environments. Here we report a cosmological hydrodynamic galaxy formation simulation that is able to form a submillimetre galaxy that simultaneously satisfies the broad range of observed physical constraints. We find that groups of galaxies residing in massive dark matter haloes have increasing rates of star formation that peak at collective rates of about 500-1,000 solar masses per year at redshifts of two to three, by which time the interstellar medium is sufficiently enriched with metals that the region may be observed as a submillimetre-selected system. The intense star formation rates are fuelled in part by the infall of a reservoir gas supply enabled by stellar feedback at earlier times, not through major mergers. With a lifetime of nearly a billion years, our simulations show that the submillimetre-bright phase of high-redshift galaxies is prolonged and associated with significant mass buildup in early-Universe proto-clusters, and that many submillimetre-bright galaxies are composed of numerous unresolved components (for which there is some observational evidence).","tags":["Authorship;DXL Member Papers"],"title":"The formation of submillimetre-bright galaxies from gas infall over a billion years","type":"publication"},{"authors":["M Turk"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"96c4421b91ab99587be7a001045f86a2","permalink":"https://matthewturk.github.io/publication/turk-2015-yw/","publishdate":"2019-05-30T20:07:09.373042Z","relpermalink":"/publication/turk-2015-yw/","section":"publication","summary":"Gmail. It serves as the center of my digital life. When I receive a new email with a Google Docs document linked in it, there's a preview of that document--- when I send one to someone else, Gmail first checks to see if the person receiving it has permission to view it and lets me know …","tags":["Authorship"],"title":"Vertical Integration","type":"publication"},{"authors":["Samuel W Skillman","Michael S Warren","Matthew J Turk","Risa H Wechsler","Daniel E Holz","P M Sutter"],"categories":null,"content":"","date":1404172800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"c4612415f473e20e2680d86b0e53ec81","permalink":"https://matthewturk.github.io/publication/skillman-2014-vt/","publishdate":"2019-05-30T20:07:09.380327Z","relpermalink":"/publication/skillman-2014-vt/","section":"publication","summary":"The Dark Sky Simulations are an ongoing series of cosmological N-body simulations designed to provide a quantitative and accessible model of the evolution of the large-scale Universe. Such models are essential for many aspects of the study of dark matter and dark energy, since we lack a sufficiently accurate analytic model of non-linear gravitational clustering. In July 2014, we made available to the general community our early data release, consisting of over 55 Terabytes of simulation data products, including our largest simulation to date, which used $1.07 times 10^12~(10240^3)$ particles in a volume $8h^-1mathrmGpc$ across. Our simulations were performed with 2HOT, a purely tree-based adaptive N-body method, running on 200,000 processors of the Titan supercomputer, with data analysis enabled by yt. We provide an overview of the derived halo catalogs, mass function, power spectra and light cone data. We show self-consistency in the mass function and mass power spectrum at the 1% level over a range of more than 1000 in particle mass. We also present a novel method to distribute and access very large datasets, based on an abstraction of the World Wide Web (WWW) as a file system, remote memory-mapped file access semantics, and a space-filling curve index. This method has been implemented for our data release, and provides a means to not only query stored results such as halo catalogs, but also to design and deploy new analysis techniques on large distributed datasets.","tags":["Authorship"],"title":"Dark Sky Simulations: Early Data Release","type":"publication"},{"authors":["M Turk"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"a1f14428b663d7f3b82f91bbfe0a9609","permalink":"https://matthewturk.github.io/publication/turk-2014-ak/","publishdate":"2019-05-30T20:07:09.364588Z","relpermalink":"/publication/turk-2014-ak/","section":"publication","summary":"Designing Software for Collaboration In computational science, collaboration in different scientific communities often takes different forms---examining the results of data generation or acquisition, combining dispa- rate pieces of software in a computational pipeline, and even enhancing …","tags":["Authorship"],"title":"Fostering Collaborative Computational Science","type":"publication"},{"authors":["Benjamin Holtzman","Jason Candler","Matthew Turk","Daniel Peter"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"ce1db4fcedcc809c4ff9dcee5ab6f2ff","permalink":"https://matthewturk.github.io/publication/holtzman-2014-wb/","publishdate":"2019-05-30T20:07:09.373678Z","relpermalink":"/publication/holtzman-2014-wb/","section":"publication","summary":"We construct a representation of earthquakes and global seismic waves through sound and animated images. The seismic wave field is the ensemble of elastic waves that propagate through the planet after an earthquake, emanating from the rupture on the fault. The sounds are made by time compression (i.e. speeding up) of seismic data with minimal additional processing. The animated images are renderings of numerical simulations of seismic wave propagation in the globe. Synchronized sounds and images reveal complex patterns and illustrate numerous aspects of the seismic wave field. These movies represent phenomena occurring far from the time and length scales normally accessible to us, creating a profound experience for the observer. The multi-sensory perception of these complex phenomena may also bring new insights to researchers.","tags":["Authorship"],"title":"Seismic Sound Lab: Sights, Sounds and Perception of the Earth as an Acoustic Space","type":"publication"},{"authors":["T Kwasnitschka","K C Yu","M Turk"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"36670fa758f1f2a192e24ec7cefa9258","permalink":"https://matthewturk.github.io/publication/kwasnitschka-2014-ya/","publishdate":"2019-05-30T20:07:09.361875Z","relpermalink":"/publication/kwasnitschka-2014-ya/","section":"publication","summary":"What topics belong inside a planetarium and what is beyond the scope of our mission? Are we limited to astronomy?","tags":["Authorship"],"title":"The Ground beath our Feet: Earth Science in the Planetarium","type":"publication"},{"authors":["Ji-Hoon Kim","Mark R Krumholz","John H Wise","Matthew J Turk","Nathan J Goldbaum","Tom Abel"],"categories":null,"content":"","date":1383264000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"7dc7f1a8d9fa48cd9148edec88aee4a6","permalink":"https://matthewturk.github.io/publication/kim-2013-ac/","publishdate":"2019-05-30T20:07:09.371064Z","relpermalink":"/publication/kim-2013-ac/","section":"publication","summary":"We investigate the spatially resolved star formation relation using a galactic disk formed in a comprehensive high-resolution (3.8 pc) simulation. Our new implementation of stellar feedback includes ionizing radiation as well as supernova explosions, and we handle ionizing radiation by solving the radiative transfer equation rather than by a subgrid model. Photoheating by stellar radiation stabilizes gas against Jeans fragmentation, reducing the star formation rate (SFR). Because we have self-consistently calculated the location of ionized gas, we are able to make simulated, spatially resolved observations of star formation tracers, such as H$α$ emission. We can also observe how stellar feedback manifests itself in the correlation between ionized and molecular gas. Applying our techniques to the disk in a galactic halo of 2.3 $times$ 1011 M ☉, we find that the correlation between SFR density (estimated from mock H$α$ emission) and H2 density shows large scatter, especially at high resolutions of ≲75 pc that are comparable to the size of giant molecular clouds (GMCs). This is because an aperture of GMC size captures only particular stages of GMC evolution and because H$α$ traces hot gas around star-forming regions and is displaced from the H2 peaks themselves. By examining the evolving environment around star clusters, we speculate that the breakdown of the traditional star formation laws of the Kennicutt-Schmidt type at small scales is further aided by a combination of stars drifting from their birthplaces and molecular clouds being dispersed via stellar feedback.","tags":["Authorship"],"title":"DWARF GALAXIES WITH IONIZING RADIATION FEEDBACK. II. SPATIALLY RESOLVED STAR FORMATION RELATION","type":"publication"},{"authors":["Ji-Hoon Kim","Mark R Krumholz","John H Wise","Matthew J Turk","Nathan J Goldbaum","Tom Abel"],"categories":null,"content":"","date":1377993600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"d89d1130804c0344ac02ec8f41dc7d61","permalink":"https://matthewturk.github.io/publication/kim-2013-jy/","publishdate":"2019-05-30T20:07:09.379476Z","relpermalink":"/publication/kim-2013-jy/","section":"publication","summary":"We describe a new method for simulating ionizing radiation and supernova feedback in the analogs of low-redshift galactic disks. In this method, which we call star-forming molecular cloud (SFMC) particles, we use a ray-tracing technique to solve the radiative transfer equation for ultraviolet photons emitted by thousands of distinct particles on the fly. Joined with high numerical resolution of 3.8 pc, the realistic description of stellar feedback helps to self-regulate star formation. This new feedback scheme also enables us to study the escape of ionizing photons from star-forming clumps and from a galaxy, and to examine the evolving environment of star-forming gas clumps. By simulating a galactic disk in a halo of 2.3 $times$ 1011 M ☉, we find that the average escape fraction from all radiating sources on the spiral arms (excluding the central 2.5 kpc) fluctuates between 0.08% and 5.9% during a ∼20 Myr period with a mean value of 1.1%. The flux of escaped photons from these sources is not strongly beamed, but manifests a large opening angle of more than 60° from the galactic pole. Further, we investigate the escape fraction per SFMC particle, f esc(i), and how it evolves as the particle ages. We discover that the average escape fraction f esc is dominated by a small number of SFMC particles with high f esc(i). On average, the escape fraction from an SFMC particle rises from 0.27% at its birth to 2.1% at the end of a particle lifetime, 6 Myr. This is because SFMC particles drift away from the dense gas clumps in which they were born, and because the gas around the star-forming clumps is dispersed by ionizing radiation and supernova feedback. The framework established in this study brings deeper insight into the physics of photon escape fraction from an individual star-forming clump and from a galactic disk.","tags":["Authorship"],"title":"DWARF GALAXIES WITH IONIZING RADIATION FEEDBACK. I. ESCAPE OF IONIZING PHOTONS","type":"publication"},{"authors":["The Enzo Collaboration","Greg L Bryan","Michael L Norman","Brian W O'Shea","Tom Abel","John H Wise","Matthew J Turk","Daniel R Reynolds","David C Collins","Peng Wang","Samuel W Skillman","Britton Smith","Robert P Harkness","James Bordner","Ji-Hoon Kim","Michael Kuhlen","Hao Xu","Nathan Goldbaum","Cameron Hummels","Alexei G Kritsuk","Elizabeth Tasker","Stephen Skory","Christine M Simpson","Oliver Hahn","Jeffrey S Oishi","Geoffrey C So","Fen Zhao","Renyue Cen","Yuan Li"],"categories":null,"content":"","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"8517285c643a11800825d1dc3fe6aa6b","permalink":"https://matthewturk.github.io/publication/the-enzo-collaboration-2013-es/","publishdate":"2019-05-30T20:07:09.391823Z","relpermalink":"/publication/the-enzo-collaboration-2013-es/","section":"publication","summary":"This paper describes the open-source code Enzo, which uses block-structured adaptive mesh refinement to provide high spatial and temporal resolution for modeling astrophysical fluid flows. The code is Cartesian, can be run in 1, 2, and 3 dimensions, and supports a wide variety of physics including hydrodynamics, ideal and non-ideal magnetohydrodynamics, N-body dynamics (and, more broadly, self-gravity of fluids and particles), primordial gas chemistry, optically-thin radiative cooling of primordial and metal-enriched plasmas (as well as some optically-thick cooling models), radiation transport, cosmological expansion, and models for star formation and feedback in a cosmological context. In addition to explaining the algorithms implemented, we present solutions for a wide range of test problems, demonstrate the code's parallel performance, and discuss the Enzo collaboration's code development methodology.","tags":["Authorship"],"title":"Enzo: An Adaptive Mesh Refinement Code for Astrophysics","type":"publication"},{"authors":["M J Turk"],"categories":null,"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"c8a33d90212b796c8d03b80111c09aa0","permalink":"https://matthewturk.github.io/publication/turk-2013-ks/","publishdate":"2019-05-30T20:07:09.374781Z","relpermalink":"/publication/turk-2013-ks/","section":"publication","summary":"As scientists' needs for computational techniques and tools grow, they cease to be supportable by software developed in isolation. In many cases, these needs are being met by communities of practice, where software is developed by domain scientists to reach …","tags":["Authorship"],"title":"How to scale a code in the human dimension","type":"publication"},{"authors":["Matthew J Turk"],"categories":null,"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"2e693ceb78643251252efd3fdb648e1b","permalink":"https://matthewturk.github.io/publication/turk-2013-jj/","publishdate":"2019-05-30T20:07:09.375168Z","relpermalink":"/publication/turk-2013-jj/","section":"publication","summary":"As scientists' needs for computational techniques and tools grow, they cease to be supportable by software developed in isolation. In many cases, these needs are being met by communities of practice, where software is developed by domain scientists to reach …","tags":["XSEDE proceedings","community","open source;Authorship"],"title":"Scaling a Code in the Human Dimension","type":"publication"},{"authors":["John H Wise","Tom Abel","Matthew J Turk","Michael L Norman","Britton D Smith"],"categories":null,"content":"","date":1346457600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"52da72f3fa7de9f835f555ba2e7cb3a4","permalink":"https://matthewturk.github.io/publication/wise-2012-kl/","publishdate":"2019-05-30T20:07:09.362758Z","relpermalink":"/publication/wise-2012-kl/","section":"publication","summary":"Here we present three adaptive mesh refinement radiation hydrodynamics simulations that illustrate the impact of momentum transfer from ionising radiation to the absorbing gas on star formation in high-redshift dwarf galaxies. Momentum transfer is calculated by solving the radiative transfer equation with a ray tracing algorithm that is adaptive in spatial and angular coordinates. We find that momentum input partially affects star formation by increasing the turbulent support to a three-dimensional rms velocity equal to the circular velocity of early haloes. Compared to a calculation that neglects radiation pressure, the star formation rate is decreased by a factor of five to 1.8 ? 10?2 M? yr?1 in a dwarf galaxy with a dark matter and stellar mass of 2.0 ? 108 M? and 4.5 ? 105 M?, respectively, when radiation pressure is included. Its mean metallicity of 10?2.1 Z? is consistent with the observed dwarf galaxy luminosity-metallicity relation. In addition to photo-heating in H II regions, radiation pressure further drives dense gas from star forming regions, so supernovae feedback occurs in a warmer and more diffuse medium, launching metal-rich outflows.","tags":["Authorship"],"title":"The imprint of pop III stars on the first galaxies","type":"publication"},{"authors":["Matthew J Turk","Jeffrey S Oishi","Tom Abel","Greg L Bryan"],"categories":null,"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"3cbbedf0bd49e08ae8ed40e5800104c8","permalink":"https://matthewturk.github.io/publication/turk-2012-ac/","publishdate":"2019-05-30T20:07:09.382091Z","relpermalink":"/publication/turk-2012-ac/","section":"publication","summary":"We study the buildup of magnetic fields during the formation of Population III star-forming regions by conducting cosmological simulations from realistic initial conditions and varying the Jeans resolution. To investigate this in detail, we start simulations from identical initial conditions, mandating 16, 32, and 64 zones per Jeans length, and study the variation in their magnetic field amplification. We find that, while compression results in some amplification, turbulent velocity fluctuations driven by the collapse can further amplify an initially weak seed field via dynamo action, provided there is sufficient numerical resolution to capture vortical motions (we find this requirement to be 64 zones per Jeans length, slightly larger than but consistent with previous work run with more idealized collapse scenarios). We explore saturation of amplification of the magnetic field, which could potentially become dynamically important in subsequent, fully resolved calculations. We have also identified a relatively surprising phenomenon that is purely hydrodynamic: the higher-resolved simulations possess substantially different characteristics, including higher infall velocity, increased temperatures inside 1000 AU, and decreased molecular hydrogen content in the innermost region. Furthermore, we find that disk formation is suppressed in higher-resolution calculations, at least at the times that we can follow the calculation. We discuss the effect this may have on the buildup of disks over the accretion history of the first clump to form as well as the potential for gravitational instabilities to develop and induce fragmentation.","tags":["Authorship"],"title":"MAGNETIC FIELDS IN POPULATION III STAR FORMATION","type":"publication"},{"authors":["John H Wise","Tom Abel","Matthew J Turk","Michael L Norman","Britton D Smith"],"categories":null,"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"aa120229066e2ea9fc4e86004a779463","permalink":"https://matthewturk.github.io/publication/wise-2012-dn/","publishdate":"2019-05-30T20:07:09.382982Z","relpermalink":"/publication/wise-2012-dn/","section":"publication","summary":"Massive stars provide feedback that shapes the interstellar medium of galaxies at all redshifts and their resulting stellar populations. Here we present three adaptive mesh refinement radiation hydrodynamics simulations that illustrate the impact of momentum …","tags":["Authorship"],"title":"The birth of a galaxy--II. The role of radiation pressure","type":"publication"},{"authors":["John H Wise","Matthew J Turk","Michael L Norman","Tom Abel"],"categories":null,"content":"","date":1322697600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"2e513dfacd2ecb326d361c9ba89cffb2","permalink":"https://matthewturk.github.io/publication/wise-2011-ph/","publishdate":"2019-05-30T20:07:09.386569Z","relpermalink":"/publication/wise-2011-ph/","section":"publication","summary":"By definition, Population III stars are metal-free, and their protostellar collapse is driven by molecular hydrogen cooling in the gas phase, leading to large characteristic masses. Population II stars with lower characteristic masses form when the star-forming gas reaches a critical metallicity of 10--6-10--3.5 Z ☉. We present an adaptive mesh refinement radiation hydrodynamics simulation that follows the transition from Population III to Population II star formation. The maximum spatial resolution of 1 comoving parsec allows for individual molecular clouds to be well resolved and their stellar associations to be studied in detail. We model stellar radiative feedback with adaptive ray tracing. A top-heavy initial mass function for the Population III stars is considered, resulting in a plausible distribution of pair-instability supernovae and associated metal enrichment. We find that the gas fraction recovers from 5% to nearly the cosmic fraction in halos with merger histories rich in halos above 107 M ☉. A single pair-instability supernova is sufficient to enrich the host halo to a metallicity floor of 10--3 Z ☉ and to transition to Population II star formation. This provides a natural explanation for the observed floor on damped Ly$α$ systems metallicities reported in the literature, which is of this order. We find that stellar metallicities do not necessarily trace stellar ages, as mergers of halos with established stellar populations can create superpositions of t--Z evolutionary tracks. A bimodal metallicity distribution is created after a starburst occurs when the halo can cool efficiently through atomic line cooling.","tags":["Authorship"],"title":"THE BIRTH OF A GALAXY: PRIMORDIAL METAL ENRICHMENT AND STELLAR POPULATIONS","type":"publication"},{"authors":["M J Turk","B D Smith"],"categories":null,"content":"","date":1293840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"b3764bc9a12880080515cba2186c14bc","permalink":"https://matthewturk.github.io/publication/turk-2011-ck/","publishdate":"2019-05-30T20:07:09.366733Z","relpermalink":"/publication/turk-2011-ck/","section":"publication","summary":"The usage of the high-level scripting language Python has enabled new mechanisms for data interrogation, discovery and visualization of scientific data. We present yt, an open source, community-developed astrophysical analysis and visualization toolkit for data …","tags":["Authorship"],"title":"High-Performance Astrophysical Simulations and Analysis with Python","type":"publication"},{"authors":["Matthew J Turk","Paul Clark","S C O Glover","T H Greif","Tom Abel","Ralf Klessen","Volker Bromm"],"categories":null,"content":"","date":1291161600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"972f8cfd264acbc97e71613f0de8f131","permalink":"https://matthewturk.github.io/publication/turk-2010-mg/","publishdate":"2019-05-30T20:07:09.381233Z","relpermalink":"/publication/turk-2010-mg/","section":"publication","summary":"The transformation of atomic hydrogen to molecular hydrogen through three-body reactions is a crucial stage in the collapse of primordial, metal-free halos, where the first generation of stars (Population III stars) in the universe is formed. However, in the published literature, the rate coefficient for this reaction is uncertain by nearly an order of magnitude. We report on the results of both adaptive mesh refinement and smoothed particle hydrodynamics simulations of the collapse of metal-free halos as a function of the value of this rate coefficient. For each simulation method, we have simulated a single halo three times, using three different values of the rate coefficient. We find that while variation between halo realizations may be greater than that caused by the three-body rate coefficient being used, both the accretion physics onto Population III protostars as well as the long-term stability of the disk and any potential fragmentation may depend strongly on this rate coefficient.","tags":["Authorship"],"title":"EFFECTS OF VARYING THE THREE-BODY MOLECULAR HYDROGEN FORMATION RATE IN PRIMORDIAL STAR FORMATION","type":"publication"},{"authors":["Matthew J Turk","Michael L Norman","Tom Abel"],"categories":null,"content":"","date":1291161600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"ba4d0d7e4ffe9e250803df6633aca598","permalink":"https://matthewturk.github.io/publication/turk-2010-vh/","publishdate":"2019-05-30T20:07:09.389768Z","relpermalink":"/publication/turk-2010-vh/","section":"publication","summary":"We report on simulations of the formation of the first stars in the","tags":["cosmology: theory; galaxies: formation; H II regions; stars:; formation; Astrophysics - Cosmology and Nongalactic Astrophysics;Authorship"],"title":"High-entropy Polar Regions Around the First Protostars","type":"publication"},{"authors":["Matthew J Turk","Britton D Smith","Jeffrey S Oishi","Stephen Skory","Samuel W Skillman","Tom Abel","Michael L Norman"],"categories":null,"content":"","date":1291161600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"880368f1545659f8c60cd5a61c8a39e5","permalink":"https://matthewturk.github.io/publication/turk-2010-hk/","publishdate":"2019-05-30T20:07:09.388606Z","relpermalink":"/publication/turk-2010-hk/","section":"publication","summary":"The analysis of complex multiphysics astrophysical simulations presents a unique and rapidly growing set of challenges: reproducibility, parallelization, and vast increases in data size and complexity chief among them. In order to meet these challenges, and in order to open up new avenues for collaboration between users of multiple simulation platforms, we present yt (available at http://yt.enzotools.org/) an open source, community-developed astrophysical analysis and visualization toolkit. Analysis and visualization with yt are oriented around physically relevant quantities rather than quantities native to astrophysical simulation codes. While originally designed for handling Enzo's structure adaptive mesh refinement data, yt has been extended to work with several different simulation methods and simulation codes including Orion, RAMSES, and FLASH. We report on its methods for reading, handling, and visualizing data, including projections, multivariate volume rendering, multi-dimensional histograms, halo finding, light cone generation, and topologically connected isocontour identification. Furthermore, we discuss the underlying algorithms yt uses for processing and visualizing data, and its mechanisms for parallelization of analysis tasks.","tags":["Authorship"],"title":"yt: A MULTI-CODE ANALYSIS TOOLKIT FOR ASTROPHYSICAL SIMULATION DATA","type":"publication"},{"authors":["S Skory","M J Turk","M L Norman","A L Coil"],"categories":null,"content":"","date":1262304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"477b00158b1a9bd5ba4c5e21832541b4","permalink":"https://matthewturk.github.io/publication/skory-2010-il/","publishdate":"2019-05-30T20:07:09.377133Z","relpermalink":"/publication/skory-2010-il/","section":"publication","summary":"Modern N-body cosmological simulations contain billions ($10^ 9$) of dark matter particles. These simulations require hundreds to thousands of gigabytes of memory, and employ hundreds to tens of thousands of processing cores on many compute nodes. In order to …","tags":["Authorship"],"title":"Parallel hop: A scalable halo finder for massive cosmological data sets","type":"publication"},{"authors":["Matthew J Turk","Tom Abel","Brian O'Shea"],"categories":null,"content":"","date":1246406400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"ecfd80b5ead375d5b6a7dee166994961","permalink":"https://matthewturk.github.io/publication/turk-2009-fl/","publishdate":"2019-05-30T20:07:09.385796Z","relpermalink":"/publication/turk-2009-fl/","section":"publication","summary":"Previous high-resolution cosmological simulations predicted that the first stars to appear in the early universe were very massive and formed in isolation. Here, we discuss a cosmological simulation in which the central 50 M(o) (where M(o) is the mass of the Sun) clump breaks up into two cores having a mass ratio of two to one, with one fragment collapsing to densities of 10(-8) grams per cubic centimeter. The second fragment, at a distance of approximately 800 astronomical units, is also optically thick to its own cooling radiation from molecular hydrogen lines but is still able to cool via collision-induced emission. The two dense peaks will continue to accrete from the surrounding cold gas reservoir over a period of approximately 10(5) years and will likely form a binary star system.","tags":["Authorship"],"title":"The formation of Population III binaries from cosmological initial conditions","type":"publication"},{"authors":["Britton D Smith","Matthew J Turk","Steinn Sigurdsson","Brian W O'Shea","Michael L Norman"],"categories":null,"content":"","date":1230768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"b1dcadcb3cbb4b53f68bf4f702bd8de8","permalink":"https://matthewturk.github.io/publication/smith-2009-dp/","publishdate":"2019-05-30T20:07:09.384512Z","relpermalink":"/publication/smith-2009-dp/","section":"publication","summary":"Simulations of the formation of Population III (Pop III) stars suggest that they were much more massive than the Pop II and Pop I stars observed today. This is due to the collapse dynamics of metal-free gas, which is regulated by the radiative cooling of molecular hydrogen. We study how the collapse of gas clouds is altered by the addition of metals to the star-forming environment by performing a series of simulations of pre-enriched star formation at various metallicities. To make a clean comparison with metal-free star formation, we use initial conditions identical to a Pop III star formation simulation, with low ionization and no external radiation other than the cosmic microwave background (CMB). For metallicities below the critical metallicity, Z cr, collapse proceeds similar to the metal-free case, and only massive objects form. For metallicities well above Z cr, efficient cooling rapidly lowers the gas temperature to the temperature of the CMB. The gas is unable to radiatively cool below the CMB temperature, and becomes thermally stable. For high metallicities, Z ≳ 10--2.5 Z ☉, this occurs early in the evolution of the gas cloud, when the density is still relatively low. The resulting cloud cores show little or no fragmentation, and would most likely form massive stars. If the metallicity is not vastly above Z cr, the cloud cools efficiently but does not reach the CMB temperature, and fragmentation into multiple objects occurs. We conclude that there were three distinct modes of star formation at high redshift (z ≳ 4): a ``primordial'' mode, producing massive stars (10s to 100s of M ☉) at very low metallicities (Z ≲ 10--3.75 Z ☉); a CMB-regulated mode, producing moderate mass (10s of M ☉) stars at high metallicities (Z ≳ 10--2.5 Z ☉ at redshift z∼ 15-20); and a low-mass (a few M ☉) mode existing between these two metallicities. As the universe ages and the CMB temperature decreases, the range of the low-mass mode extends to higher metallicities, eventually becoming the only mode of star formation.","tags":["Authorship"],"title":"THREE MODES OF METAL-ENRICHED STAR FORMATION IN THE EARLY UNIVERSE","type":"publication"},{"authors":["John H Wise","Matthew J Turk","Tom Abel"],"categories":null,"content":"","date":1228089600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"6e687ba97e6cd867bc232f39e56f3687","permalink":"https://matthewturk.github.io/publication/wise-2008-gl/","publishdate":"2019-05-30T20:07:09.383653Z","relpermalink":"/publication/wise-2008-gl/","section":"publication","summary":"Numerous cosmological hydrodynamic studies have addressed the formation of galaxies. Here we choose to study the first stages of galaxy formation, including nonequilibrium atomic primordial gas cooling, gravity, and hydrodynamics. Using initial conditions appropriate for the concordance cosmological model of structure formation, we perform two adaptive mesh refinement simulations of 108 M☉ galaxies at high redshift. The calculations resolve the Jeans length at all times with more than 16 cells and capture over 14 orders of magnitude in length scales. In both cases, the dense, 105 solar mass, one parsec central regions are found to contract rapidly and have turbulent Mach numbers up to 4. Despite the ever decreasing Jeans length of the isothermal gas, we only find one site of fragmentation during the collapse. However, rotational secular bar instabilities transport angular momentum outward in the central parsec as the gas continues to collapse and lead to multiple nested unstable fragments with decreasing masses down to sub-Jupiter mass scales. Although these numerical experiments neglect star formation and feedback, they clearly highlight the physics of turbulence in gravitationally collapsing gas. The angular momentum segregation seen in our calculations plays an important role in theories that form supermassive black holes from gaseous collapse.","tags":["Authorship"],"title":"Resolving the Formation of Protogalaxies. II. Central Gravitational Collapse","type":"publication"},{"authors":["Matthew J Turk","Tom Abel","Brian W O'Shea"],"categories":null,"content":"","date":1204329600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"41c188ffd7b199c76187c2e05e873a44","permalink":"https://matthewturk.github.io/publication/turk-2008-nk/","publishdate":"2019-05-30T20:07:09.363676Z","relpermalink":"/publication/turk-2008-nk/","section":"publication","summary":"Modeling the formation of the first stars in the universe is a well?posed problem and ideally suited for computational investigation.We have conducted high?resolution numerical studies of the formation of primordial stars. Beginning with primordial initial conditions appropriate for a ?CDM model, we used the Eulerian adaptive mesh refinement code (Enzo) to achieve unprecedented numerical resolution, resolving cosmological scales as well as sub?stellar scales simultaneously. Building on the work of Abel, Bryan and Norman (2002), we followed the evolution of the first collapsing cloud until molecular hydrogen is optically thick to cooling radiation. In addition, the calculations account for the process of collision?induced emission (CIE) and add approximations to the optical depth in both molecular hydrogen roto?vibrational cooling and CIE. Also considered are the effects of chemical heating/cooling from the formation/destruction of molecular hydrogen. We present the results of these simulations, showing the formation of a 10 Jupiter?mass protostellar core bounded by a strongly aspherical accretion shock. Accretion rates are found to be as high as one solar mass per year.","tags":["Authorship"],"title":"Towards Forming a Primordial Protostar in a Cosmological AMR Simulation","type":"publication"},{"authors":["M Turk"],"categories":null,"content":"","date":1199145600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"d444bc751f807abdbc31db0e30dd812f","permalink":"https://matthewturk.github.io/publication/turk-2008-bl/","publishdate":"2019-05-30T20:07:09.374317Z","relpermalink":"/publication/turk-2008-bl/","section":"publication","summary":"The study the origins of cosmic structure requires large-scale computer simulations beginning with well-constrained, observationally-determined, initial conditions. We use Adaptive Mesh Refinement to conduct multi-resolution simulations spanning twelve orders …","tags":["Authorship"],"title":"Analysis and visualization of multi-scale astrophysical simulations using python and numpy","type":"publication"}]